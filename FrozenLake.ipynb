{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epizod: 5/2000, średni reward z ostatnich 5 epizodów: -0.004, epsilon: 0.995\n",
      "Epizod: 10/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.990\n",
      "Epizod: 15/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.985\n",
      "Epizod: 20/2000, średni reward z ostatnich 5 epizodów: -0.004, epsilon: 0.980\n",
      "Epizod: 25/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.975\n",
      "Epizod: 30/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.970\n",
      "Epizod: 35/2000, średni reward z ostatnich 5 epizodów: 0.197, epsilon: 0.966\n",
      "Epizod: 40/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.961\n",
      "Epizod: 45/2000, średni reward z ostatnich 5 epizodów: -0.005, epsilon: 0.956\n",
      "Epizod: 50/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.951\n",
      "Epizod: 55/2000, średni reward z ostatnich 5 epizodów: -0.003, epsilon: 0.946\n",
      "Epizod: 60/2000, średni reward z ostatnich 5 epizodów: -0.003, epsilon: 0.942\n",
      "Epizod: 65/2000, średni reward z ostatnich 5 epizodów: -0.003, epsilon: 0.937\n",
      "Epizod: 70/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.932\n",
      "Epizod: 75/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.928\n",
      "Epizod: 80/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.923\n",
      "Epizod: 85/2000, średni reward z ostatnich 5 epizodów: 0.199, epsilon: 0.918\n",
      "Epizod: 90/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.914\n",
      "Epizod: 95/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.909\n",
      "Epizod: 100/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.905\n",
      "Epizod: 105/2000, średni reward z ostatnich 5 epizodów: -0.000, epsilon: 0.900\n",
      "Epizod: 110/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.896\n",
      "Epizod: 115/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.891\n",
      "Epizod: 120/2000, średni reward z ostatnich 5 epizodów: 0.198, epsilon: 0.887\n",
      "Epizod: 125/2000, średni reward z ostatnich 5 epizodów: 0.200, epsilon: 0.882\n",
      "Epizod: 130/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.878\n",
      "Epizod: 135/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.874\n",
      "Epizod: 140/2000, średni reward z ostatnich 5 epizodów: 0.199, epsilon: 0.869\n",
      "Epizod: 145/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.865\n",
      "Epizod: 150/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.861\n",
      "Epizod: 155/2000, średni reward z ostatnich 5 epizodów: -0.001, epsilon: 0.856\n",
      "Epizod: 160/2000, średni reward z ostatnich 5 epizodów: 0.198, epsilon: 0.852\n",
      "Epizod: 165/2000, średni reward z ostatnich 5 epizodów: 0.597, epsilon: 0.848\n",
      "Epizod: 170/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.844\n",
      "Epizod: 175/2000, średni reward z ostatnich 5 epizodów: 0.197, epsilon: 0.839\n",
      "Epizod: 180/2000, średni reward z ostatnich 5 epizodów: -0.002, epsilon: 0.835\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Parametry DQN\n",
    "GAMMA = 0.99        # współczynnik dyskontujący\n",
    "EPSILON_START = 1.0 # początkowe epsilon\n",
    "EPSILON_MIN = 0.01  # minimalne epsilon\n",
    "EPSILON_DECAY = 0.999\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 20000\n",
    "EPISODES = 2000\n",
    "\n",
    "# 2. Inicjalizacja środowiska FrozenLake 4x4\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False, map_name=\"4x4\")\n",
    "state_size = env.observation_space.n  # 16\n",
    "action_size = env.action_space.n      # 4\n",
    "\n",
    "# 3. Funkcja pomocnicza do transformacji stanu (indeksu) na wektor one-hot\n",
    "def one_hot_state(state, size=16):\n",
    "    vec = np.zeros(size)\n",
    "    vec[state] = 1.0\n",
    "    return vec\n",
    "\n",
    "# 4. Sieć neuronowa główna (online network)\n",
    "def build_q_network(state_size, action_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, input_shape=(state_size,), activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "# 5. Sieć „target” – do stabilizacji uczenia\n",
    "def build_target_network(model):\n",
    "    # tworzymy sieć o takiej samej architekturze i kopiujemy wagi\n",
    "    target = models.clone_model(model)\n",
    "    target.set_weights(model.get_weights())\n",
    "    return target\n",
    "\n",
    "# 6. Inicjalizujemy sieci\n",
    "q_network = build_q_network(state_size, action_size)\n",
    "target_network = build_target_network(q_network)\n",
    "\n",
    "# 7. Bufor pamięci\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# 8. Parametry epsilon\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "# 9. Główna pętla treningowa\n",
    "all_rewards = []\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    obs, info = env.reset()\n",
    "    state_vec = one_hot_state(obs, size=state_size)\n",
    "    \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not done:\n",
    "        # Wybór akcji epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            q_values = q_network.predict(state_vec[np.newaxis, :], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "        \n",
    "        # Wykonanie akcji\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        next_state_vec = one_hot_state(next_obs, size=state_size)\n",
    "        if np.all(next_obs)==np.all(state_vec):\n",
    "            reward-=0.001\n",
    "        memory.append((state_vec, action, reward, next_state_vec, done))\n",
    "        \n",
    "        # Przechodzimy do kolejnego stanu\n",
    "        state_vec = next_state_vec\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Uczenie sieci – aktualizacja wag po każdej akcji, jeśli mamy wystarczająco dużo danych\n",
    "        if len(memory) >= BATCH_SIZE:\n",
    "            # losujemy mini-batch\n",
    "            minibatch = random.sample(memory, BATCH_SIZE)\n",
    "            \n",
    "            # przygotowanie wektorów do trenowania\n",
    "            states_mb = np.array([m[0] for m in minibatch])  # [BATCH_SIZE, state_size]\n",
    "            actions_mb = np.array([m[1] for m in minibatch]) # [BATCH_SIZE]\n",
    "            rewards_mb = np.array([m[2] for m in minibatch]) # [BATCH_SIZE]\n",
    "            next_states_mb = np.array([m[3] for m in minibatch])  # [BATCH_SIZE, state_size]\n",
    "            dones_mb = np.array([m[4] for m in minibatch])   # [BATCH_SIZE]\n",
    "            \n",
    "            # Q-values z sieci głównej dla stanu kolejnego\n",
    "            q_next = q_network.predict(next_states_mb, verbose=0)\n",
    "            # Q-values z sieci docelowej (target) dla stanu kolejnego\n",
    "            q_next_target = target_network.predict(next_states_mb, verbose=0)\n",
    "            \n",
    "            # Wyliczamy docelowe wartości Q (targety)\n",
    "            q_targets = q_network.predict(states_mb, verbose=0)\n",
    "            \n",
    "            for i in range(BATCH_SIZE):\n",
    "                if dones_mb[i]:\n",
    "                    # epizod się zakończył\n",
    "                    q_targets[i, actions_mb[i]] = rewards_mb[i]\n",
    "                else:\n",
    "                    # Double DQN (opcjonalnie) – wybieramy akcję z q_next, a wartości z q_next_target:\n",
    "                    a_max = np.argmax(q_next[i])\n",
    "                    q_targets[i, actions_mb[i]] = rewards_mb[i] + GAMMA * q_next_target[i, a_max]\n",
    "            \n",
    "            # Trenujemy sieć główną\n",
    "            q_network.fit(states_mb, q_targets, epochs=1, verbose=0)\n",
    "        \n",
    "    # Eksploracja - zmniejszanie epsilon\n",
    "    if epsilon > EPSILON_MIN:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "    \n",
    "    # Co pewną liczbę epizodów (np. co 20) kopiujemy wagi do sieci target\n",
    "    if episode % 20 == 0:\n",
    "        target_network.set_weights(q_network.get_weights())\n",
    "    \n",
    "    # Monitoring\n",
    "    if (episode+1) % 5 == 0:\n",
    "        avg_reward = np.mean(all_rewards[-5:])\n",
    "        print(f\"Epizod: {episode+1}/{EPISODES}, \"\n",
    "              f\"średni reward z ostatnich 5 epizodów: {avg_reward:.3f}, \"\n",
    "              f\"epsilon: {epsilon:.3f}\")\n",
    "\n",
    "# Po treningu możemy zaobserwować, jak agent sobie radzi\n",
    "print(\"Trening zakończony!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
