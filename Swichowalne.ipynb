{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseOrientationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z orientacją robota:\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y, orientation_index]\n",
    "      - Akcje: 0=obrót w lewo o alpha, 1=obrót w prawo o alpha, 2=ruch do przodu\n",
    "    \"\"\"\n",
    "    def __init__(self, n=5, alpha=90, max_steps=100):\n",
    "        super().__init__()  # Wywołanie init z gym.Env\n",
    "\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Ilu \"kroków\" orientacji (np. 360 / 90 = 4)\n",
    "        assert 360 % alpha == 0, \"Kąt alpha musi dzielić 360 (np. 90, 45, 60).\"\n",
    "        self.num_orientations = 360 // alpha\n",
    "\n",
    "        # Przestrzeń akcji: 3 akcje\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Przestrzeń stanów (Box z 5 wymiarami):\n",
    "        #   robot_x, robot_y, target_x, target_y, orientation_index\n",
    "        low = np.array([0, 0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([n-1, n-1, n-1, n-1, self.num_orientations - 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Pola do przechowywania stanu\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.orientation = 0  # indeks 0..(num_orientations-1)\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Dodatkowe flagi\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # możliwe wywołanie, jeśli Gym >=0.26\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losujemy pozycję robota\n",
    "        self.robot_x = np.random.randint(self.n)\n",
    "        self.robot_y = np.random.randint(self.n)\n",
    "\n",
    "        # Losujemy pozycję celu (inną niż robota)\n",
    "        while True:\n",
    "            self.target_x = np.random.randint(self.n)\n",
    "            self.target_y = np.random.randint(self.n)\n",
    "            if (self.target_x != self.robot_x) or (self.target_y != self.robot_y):\n",
    "                break\n",
    "\n",
    "        # Losujemy orientację\n",
    "        self.orientation = np.random.randint(self.num_orientations)\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Zwraca krotkę (observation, reward, terminated, truncated, info).\n",
    "        W Gym <=0.25 może to być (observation, reward, done, info).\n",
    "        \"\"\"\n",
    "        self.reward = 0\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Wykonanie akcji\n",
    "        if action == 0:\n",
    "            # Obrót w lewo o alpha\n",
    "            self.orientation = (self.orientation - 1) % self.num_orientations\n",
    "        elif action == 1:\n",
    "            # Obrót w prawo o alpha\n",
    "            self.orientation = (self.orientation + 1) % self.num_orientations\n",
    "        elif action == 2:\n",
    "            # Ruch do przodu w kierunku aktualnej orientacji\n",
    "            self._move_forward()\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Niewielka kara za każdy ruch (np. -0.0001)\n",
    "        self.reward += -0.0001\n",
    "\n",
    "        new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Drobna nagroda za zbliżenie się do celu:\n",
    "        if new_distance < old_distance:\n",
    "            self.reward += 0.1\n",
    "        elif new_distance > old_distance:\n",
    "            # Kara za oddalenie się od celu\n",
    "            self.reward -= 0.1\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        # Inkrementacja kroku\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Prosta wizualizacja w konsoli.\n",
    "        'R' = robot, 'T' = cel, '.' = puste pole\n",
    "        \"\"\"\n",
    "        grid = [[\".\" for _ in range(self.n)] for _ in range(self.n)]\n",
    "\n",
    "        # Robot\n",
    "        grid[self.robot_y][self.robot_x] = \"R\"\n",
    "        # Cel\n",
    "        grid[self.target_y][self.target_x] = \"T\"\n",
    "\n",
    "        print(\"=\" * (2*self.n))\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2*self.n))\n",
    "        orientation_deg = self.orientation * self.alpha\n",
    "        print(f\"Orientacja: index={self.orientation}, kąt={orientation_deg}°\")\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Zwraca obserwację:\n",
    "           [robot_x, robot_y, target_x, target_y, orientation_index]\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            self.robot_x,\n",
    "            self.robot_y,\n",
    "            self.target_x,\n",
    "            self.target_y,\n",
    "            self.orientation\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _move_forward(self):\n",
    "        \"\"\"\n",
    "        Ruch robota do przodu o 1 kratkę w zależności od orientation.\n",
    "        Jeśli ruch poza planszę -> kara i zakończenie epizodu.\n",
    "        \"\"\"\n",
    "        angle_deg = self.orientation * self.alpha\n",
    "        angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "        dx = int(round(np.cos(angle_rad)))\n",
    "        dy = int(round(np.sin(angle_rad)))\n",
    "\n",
    "        new_x = self.robot_x + dx\n",
    "        new_y = self.robot_y + dy\n",
    "\n",
    "        # Sprawdzenie granic\n",
    "        if 0 <= new_x < self.n and 0 <= new_y < self.n:\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "        else:\n",
    "            # Kara za wyjście poza obszar i zakończenie\n",
    "            self.reward -= 0.5\n",
    "            self.terminated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseGridEnv2(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z klasycznym sterowaniem (4 akcje):\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y, dist_up, dist_down, dist_left, dist_right, dist_diag]\n",
    "      - Akcje: 0=góra, 1=dół, 2=lewo, 3=prawo\n",
    "\n",
    "    Dodatkowo:\n",
    "      - `grid` opisuje siatkę (n x n), w której 0 to wolne pole, 1 – pole zajęte.\n",
    "      - Wejście na pole o wartości 1 traktujemy jak kolizję.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 grid=None,\n",
    "                 n=5, \n",
    "                 max_steps=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Jeżeli przekazano siatkę, to z niej wyznacz rozmiar, w przeciwnym razie stwórz wolny obszar NxN\n",
    "        if grid is not None:\n",
    "            self.grid = grid\n",
    "            self.n = self.grid.shape[0]\n",
    "        else:\n",
    "            self.n = n\n",
    "            # Domyślnie tworzymy wolną siatkę, same zera\n",
    "            self.grid = np.zeros((self.n, self.n), dtype=int)\n",
    "\n",
    "        # Przestrzeń akcji: 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: 9 wymiarów (4 współrzędne + 5 sensorów)\n",
    "        low = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            self.n - 1,  # robot_x\n",
    "            self.n - 1,  # robot_y\n",
    "            self.n - 1,  # target_x\n",
    "            self.n - 1,  # target_y\n",
    "            self.n - 1,  # dist_up\n",
    "            self.n - 1,  # dist_down\n",
    "            self.n - 1,  # dist_left\n",
    "            self.n - 1,  # dist_right\n",
    "            self.n - 1,  # dist_diag\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Zmienna do śledzenia stanu (inicjalizowana w reset)\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def _distance_to_wall(self, start_x, start_y, dx, dy):\n",
    "        \"\"\"\n",
    "        Zwraca liczbę wolnych pól w kierunku (dx, dy) do najbliższej ściany (przeszkody) \n",
    "        lub krawędzi siatki, startując od (start_x, start_y).\n",
    "        \"\"\"\n",
    "        step_count = 0\n",
    "        x, y = start_x, start_y\n",
    "        while True:\n",
    "            x_next = x + dx\n",
    "            y_next = y + dy\n",
    "            # Sprawdzamy krawędź\n",
    "            if not (0 <= x_next < self.n and 0 <= y_next < self.n):\n",
    "                return step_count\n",
    "            # Sprawdzamy przeszkodę\n",
    "            if self.grid[y_next, x_next] == 1:\n",
    "                return step_count\n",
    "            step_count += 1\n",
    "            x, y = x_next, y_next\n",
    "\n",
    "    def _get_sensors(self):\n",
    "        rx, ry = self.robot_x, self.robot_y\n",
    "        dist_up    = self._distance_to_wall(rx, ry,  0, -1)\n",
    "        dist_down  = self._distance_to_wall(rx, ry,  0,  1)\n",
    "        dist_left  = self._distance_to_wall(rx, ry, -1,  0)\n",
    "        dist_right = self._distance_to_wall(rx, ry,  1,  0)\n",
    "        dist_diag  = self._distance_to_wall(rx, ry, -1, -1)  # przykład: ukos w górę-lewo\n",
    "        return np.array([dist_up, dist_down, dist_left, dist_right, dist_diag], dtype=np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Podstawowe obserwacje (4 wartości)\n",
    "        podstawowe = np.array([self.robot_x, self.robot_y, \n",
    "                               self.target_x, self.target_y], dtype=np.float32)\n",
    "        # Sensory (5 wartości)\n",
    "        sensory = self._get_sensors()\n",
    "        # Łączymy w jeden wektor stanu\n",
    "        return np.concatenate([podstawowe, sensory])\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losowanie pozycji robota (tylko na wolnym polu)\n",
    "        while True:\n",
    "            rx = np.random.randint(self.n)\n",
    "            ry = np.random.randint(self.n)\n",
    "            if self.grid[ry, rx] == 0:  # wolne pole\n",
    "                self.robot_x = rx\n",
    "                self.robot_y = ry\n",
    "                break\n",
    "\n",
    "        # Losowanie pozycji celu (tylko na wolnym polu i różne od pozycji robota)\n",
    "        while True:\n",
    "            tx = np.random.randint(self.n)\n",
    "            ty = np.random.randint(self.n)\n",
    "            if (self.grid[ty, tx] == 0 and\n",
    "                (tx != self.robot_x or ty != self.robot_y)):\n",
    "                self.target_x = tx\n",
    "                self.target_y = ty\n",
    "                break\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Potencjalna nowa pozycja\n",
    "        if action == 0:   # góra\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y - 1\n",
    "        elif action == 1: # dół\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y + 1\n",
    "        elif action == 2: # lewo\n",
    "            new_x = self.robot_x - 1\n",
    "            new_y = self.robot_y\n",
    "        elif action == 3: # prawo\n",
    "            new_x = self.robot_x + 1\n",
    "            new_y = self.robot_y\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Sprawdzenie warunków kolizji lub wyjścia poza obszar\n",
    "        if not (0 <= new_x < self.n and 0 <= new_y < self.n):\n",
    "            # Kara za wyjście poza obszar\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        elif self.grid[new_y, new_x] == 1:\n",
    "            # Kara za kolizję z polem zajętym\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        else:\n",
    "            # Jeżeli pole jest wolne, aktualizujemy pozycję\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "            # Domyślna, niewielka kara za ruch\n",
    "            self.reward = -0.0001\n",
    "\n",
    "            new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                           self.robot_y - self.target_y])\n",
    "            # Nagroda/kara za przybliżenie/oddalenie się od celu\n",
    "            if new_distance < old_distance:\n",
    "                self.reward += 0.1\n",
    "            elif new_distance > old_distance:\n",
    "                self.reward -= 0.1\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Tekstowa wizualizacja środowiska:\n",
    "        'R' = robot, 'T' = cel, 'X' = przeszkoda, '.' = puste pole.\n",
    "        \"\"\"\n",
    "        grid_render = []\n",
    "        for y in range(self.n):\n",
    "            row_symbols = []\n",
    "            for x in range(self.n):\n",
    "                if self.grid[y, x] == 1:\n",
    "                    symbol = \"X\"\n",
    "                else:\n",
    "                    symbol = \".\"\n",
    "                \n",
    "                if x == self.robot_x and y == self.robot_y:\n",
    "                    symbol = \"R\"\n",
    "                if x == self.target_x and y == self.target_y:\n",
    "                    symbol = \"T\"\n",
    "                row_symbols.append(symbol)\n",
    "            grid_render.append(row_symbols)\n",
    "\n",
    "        print(\"=\" * (2 * self.n))\n",
    "        for row in grid_render:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2 * self.n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseGridEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z klasycznym sterowaniem (4 akcje):\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y]\n",
    "      - Akcje: 0=góra, 1=dół, 2=lewo, 3=prawo\n",
    "    \"\"\"\n",
    "    def __init__(self, n=5, max_steps=100):\n",
    "        super().__init__()  # wywołanie __init__ z gym.Env\n",
    "\n",
    "        self.n = n\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Przestrzeń akcji: 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Stan: (robot_x, robot_y, target_x, target_y)\n",
    "        low = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([n-1, n-1, n-1, n-1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losowanie pozycji robota\n",
    "        self.robot_x = np.random.randint(self.n)\n",
    "        self.robot_y = np.random.randint(self.n)\n",
    "\n",
    "        # Losowanie pozycji celu\n",
    "        while True:\n",
    "            self.target_x = np.random.randint(self.n)\n",
    "            self.target_y = np.random.randint(self.n)\n",
    "            if (self.target_x != self.robot_x) or (self.target_y != self.robot_y):\n",
    "                break\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        if action == 0:   # góra\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y - 1\n",
    "        elif action == 1: # dół\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y + 1\n",
    "        elif action == 2: # lewo\n",
    "            new_x = self.robot_x - 1\n",
    "            new_y = self.robot_y\n",
    "        elif action == 3: # prawo\n",
    "            new_x = self.robot_x + 1\n",
    "            new_y = self.robot_y\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Sprawdzenie granic\n",
    "        if not (0 <= new_x < self.n and 0 <= new_y < self.n):\n",
    "            # Kara za wyjście poza obszar\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        else:\n",
    "            # Aktualizacja pozycji\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "            # Domyślna niewielka kara za ruch\n",
    "            self.reward = -0.0001\n",
    "\n",
    "            new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                           self.robot_y - self.target_y])\n",
    "            # Nagroda/kara za przybliżenie/oddalenie\n",
    "            if new_distance < old_distance:\n",
    "                self.reward += 0.1\n",
    "            elif new_distance > old_distance:\n",
    "                self.reward -= 0.1\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Tekstowa wizualizacja środowiska:\n",
    "        'R' = robot, 'T' = cel, '.' = puste pole.\n",
    "        \"\"\"\n",
    "        grid = [[\".\" for _ in range(self.n)] for _ in range(self.n)]\n",
    "        grid[self.robot_y][self.robot_x] = \"R\"\n",
    "        grid[self.target_y][self.target_x] = \"T\"\n",
    "\n",
    "        print(\"=\" * (2 * self.n))\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2 * self.n))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([self.robot_x, self.robot_y,\n",
    "                         self.target_x, self.target_y], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseGridEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z klasycznym sterowaniem:\n",
    "      - Stan: [r_x, r_y, t_x, t_y]\n",
    "      - Akcje:\n",
    "        0 = góra (y -= 1)\n",
    "        1 = dół (y += 1)\n",
    "        2 = lewo (x -= 1)\n",
    "        3 = prawo (x += 1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n=5, max_steps=100):\n",
    "        super(WarehouseGridEnv, self).__init__()\n",
    "        \n",
    "        self.n = n\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Przestrzeń akcji: 4 akcje\n",
    "        #  0: góra\n",
    "        #  1: dół\n",
    "        #  2: lewo\n",
    "        #  3: prawo\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Przestrzeń stanów (r_x, r_y, t_x, t_y) - wszystko w [0, n-1]\n",
    "        low = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([n-1, n-1, n-1, n-1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Pola do przechowywania stanu\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.reward = 0\n",
    "        self.terminated = False\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        \n",
    "        # Losujemy pozycję robota\n",
    "        self.robot_x = np.random.randint(self.n)\n",
    "        self.robot_y = np.random.randint(self.n)\n",
    "        self.reward = 0\n",
    "        \n",
    "        # Losujemy pozycję celu (inną niż robota)\n",
    "        while True:\n",
    "            self.target_x = np.random.randint(self.n)\n",
    "            self.target_y = np.random.randint(self.n)\n",
    "            if (self.target_x != self.robot_x) or (self.target_y != self.robot_y):\n",
    "                break\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "\n",
    "        # Odległość przed akcją\n",
    "        old_distance = np.linalg.norm([\n",
    "            self.robot_x - self.target_x,\n",
    "            self.robot_y - self.target_y\n",
    "        ])\n",
    "\n",
    "        # Wykonujemy ruch w zależności od akcji\n",
    "        if action == 0:   # góra\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y - 1\n",
    "        elif action == 1: # dół\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y + 1\n",
    "        elif action == 2: # lewo\n",
    "            new_x = self.robot_x - 1\n",
    "            new_y = self.robot_y\n",
    "        elif action == 3: # prawo\n",
    "            new_x = self.robot_x + 1\n",
    "            new_y = self.robot_y\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Sprawdzamy, czy wychodzimy poza magazyn\n",
    "        if not (0 <= new_x < self.n and 0 <= new_y < self.n):\n",
    "            # Kara za wyjście poza obszar + zakończenie epizodu\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        else:\n",
    "            # Aktualizujemy pozycję\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "\n",
    "        # Domyślna niewielka kara (koszt) za ruch\n",
    "        self.reward += -0.0001\n",
    "\n",
    "        # Nowa odległość\n",
    "        new_distance = np.linalg.norm([\n",
    "            self.robot_x - self.target_x,\n",
    "            self.robot_y - self.target_y\n",
    "        ])\n",
    "\n",
    "        # Nagroda/kara w zależności od tego, czy zbliżyliśmy się do celu\n",
    "        if not self.terminated:  # Tylko jeśli jeszcze nie zakończono\n",
    "            if new_distance < old_distance:\n",
    "                self.reward += 0.1   # przybliżenie\n",
    "            elif new_distance > old_distance:\n",
    "                self.reward -= 0.1   # oddalenie\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        # Inkrementujemy krok i sprawdzamy limit\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Prosta wizualizacja tekstowa w konsoli.\n",
    "        'R' = robot\n",
    "        'T' = cel\n",
    "        '.' = puste\n",
    "        \"\"\"\n",
    "        grid = [[\".\" for _ in range(self.n)] for _ in range(self.n)]\n",
    "\n",
    "        # Pozycja robota\n",
    "        grid[self.robot_y][self.robot_x] = \"R\"\n",
    "        # Pozycja celu\n",
    "        grid[self.target_y][self.target_x] = \"T\"\n",
    "\n",
    "        print(\"=\"*(2*self.n))\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\"*(2*self.n))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Zwracamy obserwację (r_x, r_y, t_x, t_y).\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            self.robot_x,\n",
    "            self.robot_y,\n",
    "            self.target_x,\n",
    "            self.target_y\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# =============================\n",
    "# 1. Definicje Stałych i Hiperparametrów\n",
    "# =============================\n",
    "\n",
    "GAMMA = 0.9\n",
    "EPSILON_START = 0.2\n",
    "EPSILON_MIN = 0.0\n",
    "EPSILON_DECAY = 0.9  # Zmniejszanie epsilon stopniowo\n",
    "BATCH_SIZE = 24 # Typowy rozmiar batcha dla DQN\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 200000    # Rozmiar pamięci replay\n",
    "EPISODES = 200000\n",
    "\n",
    "# =============================\n",
    "# 2. Funkcja Wczytująca Mapę\n",
    "# =============================\n",
    "\n",
    "def wczytaj_cyfry_z_pliku_jako_macierz(nazwa_pliku):\n",
    "    macierz = []\n",
    "    with open(nazwa_pliku, 'r') as plik:\n",
    "        for linia in plik:\n",
    "            linia = linia.strip()\n",
    "            if not linia:\n",
    "                continue\n",
    "            wiersz = [int(znak) for znak in linia]\n",
    "            macierz.append(wiersz)\n",
    "    return macierz\n",
    "\n",
    "# Przykładowa mapa (ręcznie):\n",
    "my_grid = np.array([\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "    [0,0,0,0,0,0,0,0],\n",
    "])\n",
    "print(\"Mapa załadowana:\\n\", my_grid)\n",
    "\n",
    "# =============================\n",
    "# 3. Wybór Środowiska\n",
    "# =============================\n",
    "\n",
    "ENV_TYPE = \"grid\"  # \"orientation\" lub \"grid\"\n",
    "\n",
    "if ENV_TYPE == \"orientation\":\n",
    "    env = WarehouseOrientationEnv(n=64, alpha=90, max_steps=200)\n",
    "else:\n",
    "    env = WarehouseGridEnv2(n=8, max_steps=32, grid=my_grid)\n",
    "\n",
    "# =============================\n",
    "# 4. Standaryzacja Stanów\n",
    "# =============================\n",
    "\n",
    "MEANS = {\n",
    "    'x': 4.5,\n",
    "    'y':  4.5,\n",
    "    'tx':  4.5,\n",
    "    'ty':  4.5,\n",
    "}\n",
    "STDS = {\n",
    "    'x': 5.25,\n",
    "    'y': 5.25,\n",
    "    'tx': 5.25,\n",
    "    'ty': 5.25,\n",
    "}\n",
    "\n",
    "def transform_state_orientation(obs):\n",
    "    \"\"\"\n",
    "    Zakładamy obs ma postać (5,):\n",
    "      [x, y, tx, ty, orientation]   gdzie orientation ∈ {0,1,2,3}\n",
    "    Zwracamy [x_norm, y_norm, tx_norm, ty_norm, sin(theta), cos(theta)]\n",
    "    \"\"\"\n",
    "    obs = np.array(obs, dtype=float)\n",
    "    # Standaryzacja współrzędnych\n",
    "    x = (obs[0] - MEANS['x']) / STDS['x']\n",
    "    y = (obs[1] - MEANS['y']) / STDS['y']\n",
    "    tx = (obs[2] - MEANS['tx']) / STDS['tx']\n",
    "    ty = (obs[3] - MEANS['ty']) / STDS['ty']\n",
    "    orientation = obs[4]\n",
    "    angle_deg = orientation * 90.0\n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    sin_v = np.sin(angle_rad)\n",
    "    cos_v = np.cos(angle_rad)\n",
    "    new_obs = np.array([x, y, tx, ty, sin_v, cos_v], dtype=float)\n",
    "    return new_obs\n",
    "\n",
    "def transform_state_grid(obs):\n",
    "    \"\"\"\n",
    "    Zakładamy obs ma postać (4,) lub więcej, np. [x, y, tx, ty, ...].\n",
    "    Tu dla przykładu weźmy tylko x, y, tx, ty.\n",
    "    \"\"\"\n",
    "    obs = np.array(obs, dtype=float)\n",
    "    x = (obs[0] - MEANS['x']) / STDS['x']\n",
    "    y = (obs[1] - MEANS['y']) / STDS['y']\n",
    "    tx = (obs[2] - MEANS['tx']) / STDS['tx']\n",
    "    ty = (obs[3] - MEANS['ty']) / STDS['ty']\n",
    "    new_obs = np.array([x, y, tx, ty], dtype=float)\n",
    "    return new_obs\n",
    "\n",
    "if ENV_TYPE == \"orientation\":\n",
    "    transform_state_fn = transform_state_orientation\n",
    "    state_size = 6\n",
    "else:\n",
    "    transform_state_fn = transform_state_grid\n",
    "    state_size = 4\n",
    "\n",
    "# =============================\n",
    "# 5. Definicje Modelu Q-network i Target Network\n",
    "# =============================\n",
    "\n",
    "def build_q_network(state_size, action_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_shape=(state_size,), activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(128,  activation='relu'))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    return model\n",
    "\n",
    "def build_target_network(model):\n",
    "    target = models.clone_model(model)\n",
    "    target.set_weights(model.get_weights())\n",
    "    return target\n",
    "\n",
    "# =============================\n",
    "# 6. Inicjalizacja Pamięci Replay\n",
    "# =============================\n",
    "\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "\n",
    "# =============================\n",
    "# 7. Inicjalizacja Epsilon i Monitoringu\n",
    "# =============================\n",
    "\n",
    "epsilon = EPSILON_START\n",
    "all_rewards = []\n",
    "loss_values = []\n",
    "\n",
    "# Zmienna do wyliczania uśrednionego Loss\n",
    "smoothed_loss_values =[0] \n",
    "\n",
    "# Zmienna do wyliczania normy wag\n",
    "weight_values = []\n",
    "\n",
    "# Zmienna do wyliczania uśrednionej nagrody\n",
    "smoothed_reward_values = []\n",
    "\n",
    "# =============================\n",
    "# 8. Ustalenie action_size\n",
    "# =============================\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# =============================\n",
    "# 9. Inicjalizacja Sieci Q i Sieci Target\n",
    "# =============================\n",
    "\n",
    "q_network = build_q_network(state_size, action_size)\n",
    "target_network = build_target_network(q_network)\n",
    "\n",
    "# Kompilacja sieci (aby można było np. wywoływać model(...) w trybie treningowym)\n",
    "optimizer = optimizers.Adam(learning_rate=LEARNING_RATE,)\n",
    "loss_function = MeanSquaredError()\n",
    "\n",
    "# =============================\n",
    "# 10. Przygotowanie osobnych wykresów\n",
    "# =============================\n",
    "\n",
    "# 1) Wykres Loss\n",
    "fig_loss = go.FigureWidget()\n",
    "fig_loss.add_scatter(y=smoothed_loss_values, mode='lines', name='Loss (MA)')\n",
    "fig_loss.update_layout(\n",
    "    title='Training: Loss (Moving Average)',\n",
    "    xaxis_title='Training Steps',\n",
    "    yaxis_title='Loss',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# 2) Wykres Weight (norma wag)\n",
    "fig_weight = go.FigureWidget()\n",
    "fig_weight.add_scatter(y=weight_values, mode='lines', name='WeightNorm')\n",
    "fig_weight.update_layout(\n",
    "    title='Training: Weight Norm',\n",
    "    xaxis_title='Training Steps',\n",
    "    yaxis_title='WeightNorm',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "# 3) Wykres Reward (uśredniony)\n",
    "fig_reward = go.FigureWidget()\n",
    "fig_reward.add_scatter(y=smoothed_reward_values, mode='lines', name='Reward (MA)')\n",
    "fig_reward.update_layout(\n",
    "    title='Training: Reward (Moving Average)',\n",
    "    xaxis_title='Episode',\n",
    "    yaxis_title='Reward',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "display(fig_loss, fig_weight, fig_reward)\n",
    "\n",
    "# =============================\n",
    "# 11. Główna Pętla Treningowa\n",
    "# =============================\n",
    "\n",
    "training_steps = 0\n",
    "window_size = 50  # wielkość okna do średniej kroczącej\n",
    "\n",
    "for episode in range(1, EPISODES + 1):\n",
    "    obs = env.reset()                      \n",
    "    obs = transform_state_fn(obs)          \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Epsilon-greedy\n",
    "        if np.random.rand() < epsilon or len(memory)<4000:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            q_values = q_network.predict(obs[np.newaxis, :], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "        # Wykonanie akcji w środowisku\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = transform_state_fn(next_obs)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Zapis do pamięci\n",
    "        memory.append((obs, action, reward, next_obs, terminated))\n",
    "        obs = next_obs\n",
    "\n",
    "    # Uczenie, jeśli mamy dostatecznie dużą pamięć\n",
    "    if len(memory) >= 4000:\n",
    "        # 1. Pobieramy batch z pamięci (losowe próbki)\n",
    "        minibatch = random.sample(memory, BATCH_SIZE)\n",
    "\n",
    "        # 2. Konwertujemy próbki do macierzy\n",
    "        states_mb      = np.array([m[0] for m in minibatch])\n",
    "        actions_mb     = np.array([m[1] for m in minibatch])\n",
    "        rewards_mb     = np.array([m[2] for m in minibatch])\n",
    "        next_states_mb = np.array([m[3] for m in minibatch])\n",
    "        dones_mb       = np.array([m[4] for m in minibatch])\n",
    "\n",
    "        # 3. Przewidywania sieci target i sieci Q\n",
    "        q_next = target_network.predict(next_states_mb, verbose=0)\n",
    "        q_targets = q_network.predict(states_mb, verbose=0)\n",
    "\n",
    "        # 4. Ustalenie docelowych wartości Q (q_targets)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones_mb[i]:\n",
    "                q_targets[i, actions_mb[i]] = rewards_mb[i]\n",
    "            else:\n",
    "                q_targets[i, actions_mb[i]] = rewards_mb[i] + GAMMA * np.max(q_next[i])\n",
    "\n",
    "        # 5. Pojedynczy krok uczenia (Gradient Descent)\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = q_network(states_mb, training=True)\n",
    "            loss_value = loss_function(q_targets, predictions)\n",
    "            loss_values.append(loss_value.numpy())\n",
    "        gradients = tape.gradient(loss_value, q_network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "        w_norm = 0.0\n",
    "        for var in q_network.trainable_variables:\n",
    "            w_norm += np.sum(var.numpy()**2)\n",
    "            w_norm = np.sqrt(w_norm)\n",
    "        weight_values.append(w_norm)\n",
    "        # 6. Logowanie\n",
    "       \n",
    "            # ==============================\n",
    "            # Trenowanie sieci \"ręcznie\" z użyciem GradientTape\n",
    "            # ==============================\n",
    "            # with tf.GradientTape() as tape:\n",
    "            #     predictions = q_network(states_mb, training=True)\n",
    "            #     loss = loss_function(q_targets, predictions)\n",
    "\n",
    "            # gradients = tape.gradient(loss, q_network.trainable_variables)\n",
    "            # optimizer.apply_gradients(zip(gradients, q_network.trainable_variables))\n",
    "\n",
    "            # # --- Zapis i obliczanie średniej kroczącej dla Loss ---\n",
    "            # loss_value = loss.numpy()\n",
    "            # loss_values.append(loss_value)\n",
    "        \n",
    "    if len(loss_values) < window_size:\n",
    "        ma_loss = np.mean(loss_values)\n",
    "    else:\n",
    "        ma_loss = np.mean(loss_values[-window_size:])\n",
    "    smoothed_loss_values.append(ma_loss)\n",
    "\n",
    "    # --- Obliczanie normy wag (np. L2 wszystkich wag) ---\n",
    "   \n",
    "\n",
    "    # --- Aktualizacja dwóch wykresów w czasie rzeczywistym ---\n",
    "    with fig_loss.batch_update():\n",
    "        fig_loss.data[0].y = smoothed_loss_values\n",
    "        fig_loss.update_layout(xaxis=dict(range=[0, len(smoothed_loss_values)]))\n",
    "\n",
    "    with fig_weight.batch_update():\n",
    "        fig_weight.data[0].y = weight_values\n",
    "        fig_weight.update_layout(xaxis=dict(range=[0, len(weight_values)]))\n",
    "\n",
    "    # Zmniejszamy epsilon\n",
    "    if epsilon > EPSILON_MIN:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(epsilon, EPSILON_MIN)\n",
    "\n",
    "    # Zapisujemy nagrodę za epizod\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # Okresowe kopiowanie wag do sieci target\n",
    "    #tau = 0.02  # przykładowa wartość\n",
    "    target_weights = target_network.get_weights()\n",
    "    source_weights = q_network.get_weights()\n",
    "    new_weights = []\n",
    "\n",
    "    # Mieszamy wagi: theta_target := tau * theta_source + (1 - tau) * theta_target\n",
    "    if episode % 50 == 0:\n",
    "       target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    # Co pewną liczbę epizodów wypisz informacje i aktualizuj wykres nagrody\n",
    "    if episode % 2 == 0:\n",
    "        avg_reward = np.mean(all_rewards[-1:])\n",
    "        print(f\"Epizod: {episode}/{EPISODES}, \"\n",
    "              f\"AVG ostatnich 50 ep.: {avg_reward:.3f}, \"\n",
    "              f\"epsilon: {epsilon:.3f}, \")\n",
    "\n",
    "    # --- Aktualizacja wykresu nagrody po KAŻDYM zakończonym epizodzie ---\n",
    "    if len(all_rewards) < window_size:\n",
    "        ma_reward = np.mean(all_rewards)\n",
    "    else:\n",
    "        ma_reward = np.mean(all_rewards[-window_size:])\n",
    "    smoothed_reward_values.append(ma_reward)\n",
    "\n",
    "    with fig_reward.batch_update():\n",
    "        fig_reward.data[0].y = smoothed_reward_values\n",
    "        fig_reward.update_layout(xaxis=dict(range=[0, len(smoothed_reward_values)]))\n",
    "\n",
    "print(\"Trening zakończony!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapa załadowana:\n",
      " [[0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0]\n",
      " [0 0 1 1 1 1 0 0]\n",
      " [0 0 1 1 1 1 0 0]\n",
      " [0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Filip\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ae7e791cd4498a94e06cb1333b177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines', 'name': 'Loss', 'type': 'scatter', 'uid': '6c4d2dcd-9f32-4909-88a2-d24a9976ecfb', 'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training: Loss'},\n",
       "               'xaxis': {'title': {'text': 'Training Steps'}},\n",
       "               'yaxis': {'title': {'text': 'Loss'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc439d994aad411fa8ab4b15c33d9df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'lines',\n",
       "              'name': 'GradNorm',\n",
       "              'type': 'scatter',\n",
       "              'uid': '8d0f5795-f285-47d9-9ea2-81b261d50355',\n",
       "              'y': []}],\n",
       "    'layout': {'template': '...',\n",
       "               'title': {'text': 'Training: Gradient Norm'},\n",
       "               'xaxis': {'title': {'text': 'Training Steps'}},\n",
       "               'yaxis': {'title': {'text': 'GradNorm'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 211\u001b[0m\n\u001b[0;32m    207\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (terminated \u001b[38;5;129;01mor\u001b[39;00m truncated):\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;66;03m# Epsilon-greedy\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m<\u001b[39m epsilon \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mmemory\u001b[49m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m34000\u001b[39m:\n\u001b[0;32m    212\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(action_size)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'memory' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Wczytujemy nasz wytrenowany model\n",
    "new_model = tf.keras.models.load_model('trained_dqn_model.h5')\n",
    "trained_model = new_model\n",
    "\n",
    "def test_agent(env, model, n_episodes=5):\n",
    "    \"\"\"\n",
    "    Wykonujemy n_episodes epizodów i zapisujemy trace w postaci:\n",
    "    (obs, action, reward, next_obs) dla każdego kroku.\n",
    "    \"\"\"\n",
    "    all_episode_rewards = []\n",
    "    all_traces = []  # aby przechować szczegółowe info do ewentualnej wizualizacji\n",
    "\n",
    "    for episode_i in range(n_episodes):\n",
    "        obs_5d = env.reset()\n",
    "        obs_6d = transform_state(obs_5d)  # Funkcja jak w Twoim kodzie\n",
    "\n",
    "        episode_reward = 0\n",
    "        episode_trace = []\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            # Wybór akcji - tutaj greedy, bo chcemy pokazać, co robi nauczony agent\n",
    "            q_values = model.predict(obs_6d[np.newaxis, :], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "            # Krok w środowisku\n",
    "            next_obs_5d, reward, terminated, truncated = env.step(action)\n",
    "            next_obs_6d = transform_state(next_obs_5d)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Zapis do trace\n",
    "            episode_trace.append({\n",
    "                'obs': obs_5d,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_obs': next_obs_5d\n",
    "            })\n",
    "\n",
    "            # Przechodzimy do następnego stanu\n",
    "            obs_5d = next_obs_5d\n",
    "            obs_6d = next_obs_6d\n",
    "\n",
    "        all_episode_rewards.append(episode_reward)\n",
    "        all_traces.append(episode_trace)\n",
    "\n",
    "    # Podsumowanie wyników\n",
    "    avg_reward = np.mean(all_episode_rewards)\n",
    "    print(f\"Test: średni reward z {n_episodes} epizodów = {avg_reward:.2f}\")\n",
    "\n",
    "    return all_episode_rewards, all_traces\n",
    "\n",
    "# Uruchamiamy testy\n",
    "env = WarehouseOrientationEnv(n=64, max_steps=200)\n",
    "test_rewards, traces = test_agent(env, trained_model, n_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_episode(episode_trace):\n",
    "    \"\"\"\n",
    "    Wizualizuje pozycje agenta i celu w kolejnych krokach epizodu.\n",
    "    Załóżmy, że `episode_trace` to lista słowników, gdzie:\n",
    "        step['obs'] = [agent_x, agent_y, target_x, target_y, ...]\n",
    "    \"\"\"\n",
    "\n",
    "    # Listy do przechowania kolejnych pozycji agenta i celu\n",
    "    agent_positions = []\n",
    "    target_positions = []\n",
    "    a=print(episode_trace)\n",
    "    for step in episode_trace:\n",
    "        # Wyciągamy z obs: [agent_x, agent_y, target_x, target_y, ...]\n",
    "      agent_x, agent_y   = step['obs'][0], step['obs'][1]\n",
    "      target_x, target_y = step['obs'][2], step['obs'][3]\n",
    "\n",
    "      agent_positions.append((agent_x, agent_y))\n",
    "      target_positions.append((target_x, target_y))\n",
    "\n",
    "    # Rozbijamy na osobne listy (x, y) — jeśli target się porusza, to zobaczymy jego ścieżkę\n",
    "    agent_xs  = [pos[0] for pos in agent_positions]\n",
    "    agent_ys  = [pos[1] for pos in agent_positions]\n",
    "    target_xs = [pos[0] for pos in target_positions]\n",
    "    target_ys = [pos[1] for pos in target_positions]\n",
    "\n",
    "    # Rysujemy ścieżkę agenta\n",
    "    plt.plot(agent_xs, agent_ys, 'bo-', label='Agent')\n",
    "\n",
    "    # Rysujemy ścieżkę (lub pozycje) celu\n",
    "    # Jeśli cel jest statyczny, zobaczysz same nakładające się punkty.\n",
    "    plt.plot(target_xs, target_ys, 'rx--', label='Cel')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.title('Ścieżka agenta i pozycja celu')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Załóżmy, że mamy listę epizodów `traces`,\n",
    "# gdzie np. traces[0] to trace z pierwszego epizodu\n",
    "episode_trace = traces[0]  # bierzemy pierwszy epizod z listy\n",
    "\n",
    "# Wywołujemy wizualizację\n",
    "visualize_episode(episode_trace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
