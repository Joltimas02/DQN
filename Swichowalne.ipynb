{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseOrientationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z orientacją robota:\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y, orientation_index]\n",
    "      - Akcje: 0=obrót w lewo o alpha, 1=obrót w prawo o alpha, 2=ruch do przodu\n",
    "    \"\"\"\n",
    "    def __init__(self, n=5, alpha=90, max_steps=100):\n",
    "        super().__init__()  # Wywołanie init z gym.Env\n",
    "\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Ilu \"kroków\" orientacji (np. 360 / 90 = 4)\n",
    "        assert 360 % alpha == 0, \"Kąt alpha musi dzielić 360 (np. 90, 45, 60).\"\n",
    "        self.num_orientations = 360 // alpha\n",
    "\n",
    "        # Przestrzeń akcji: 3 akcje\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Przestrzeń stanów (Box z 5 wymiarami):\n",
    "        #   robot_x, robot_y, target_x, target_y, orientation_index\n",
    "        low = np.array([0, 0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([n-1, n-1, n-1, n-1, self.num_orientations - 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Pola do przechowywania stanu\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.orientation = 0  # indeks 0..(num_orientations-1)\n",
    "        self.current_step = 0\n",
    "\n",
    "        # Dodatkowe flagi\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)  # możliwe wywołanie, jeśli Gym >=0.26\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losujemy pozycję robota\n",
    "        self.robot_x = np.random.randint(self.n)\n",
    "        self.robot_y = np.random.randint(self.n)\n",
    "\n",
    "        # Losujemy pozycję celu (inną niż robota)\n",
    "        while True:\n",
    "            self.target_x = np.random.randint(self.n)\n",
    "            self.target_y = np.random.randint(self.n)\n",
    "            if (self.target_x != self.robot_x) or (self.target_y != self.robot_y):\n",
    "                break\n",
    "\n",
    "        # Losujemy orientację\n",
    "        self.orientation = np.random.randint(self.num_orientations)\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Zwraca krotkę (observation, reward, terminated, truncated, info).\n",
    "        W Gym <=0.25 może to być (observation, reward, done, info).\n",
    "        \"\"\"\n",
    "        self.reward = 0\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Wykonanie akcji\n",
    "        if action == 0:\n",
    "            # Obrót w lewo o alpha\n",
    "            self.orientation = (self.orientation - 1) % self.num_orientations\n",
    "        elif action == 1:\n",
    "            # Obrót w prawo o alpha\n",
    "            self.orientation = (self.orientation + 1) % self.num_orientations\n",
    "        elif action == 2:\n",
    "            # Ruch do przodu w kierunku aktualnej orientacji\n",
    "            self._move_forward()\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Niewielka kara za każdy ruch (np. -0.0001)\n",
    "        self.reward += -0.0001\n",
    "\n",
    "        new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Drobna nagroda za zbliżenie się do celu:\n",
    "        if new_distance < old_distance:\n",
    "            self.reward += 0.1\n",
    "        elif new_distance > old_distance:\n",
    "            # Kara za oddalenie się od celu\n",
    "            self.reward -= 0.1\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        # Inkrementacja kroku\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Prosta wizualizacja w konsoli.\n",
    "        'R' = robot, 'T' = cel, '.' = puste pole\n",
    "        \"\"\"\n",
    "        grid = [[\".\" for _ in range(self.n)] for _ in range(self.n)]\n",
    "\n",
    "        # Robot\n",
    "        grid[self.robot_y][self.robot_x] = \"R\"\n",
    "        # Cel\n",
    "        grid[self.target_y][self.target_x] = \"T\"\n",
    "\n",
    "        print(\"=\" * (2*self.n))\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2*self.n))\n",
    "        orientation_deg = self.orientation * self.alpha\n",
    "        print(f\"Orientacja: index={self.orientation}, kąt={orientation_deg}°\")\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Zwraca obserwację:\n",
    "           [robot_x, robot_y, target_x, target_y, orientation_index]\n",
    "        \"\"\"\n",
    "        return np.array([\n",
    "            self.robot_x,\n",
    "            self.robot_y,\n",
    "            self.target_x,\n",
    "            self.target_y,\n",
    "            self.orientation\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _move_forward(self):\n",
    "        \"\"\"\n",
    "        Ruch robota do przodu o 1 kratkę w zależności od orientation.\n",
    "        Jeśli ruch poza planszę -> kara i zakończenie epizodu.\n",
    "        \"\"\"\n",
    "        angle_deg = self.orientation * self.alpha\n",
    "        angle_rad = np.deg2rad(angle_deg)\n",
    "\n",
    "        dx = int(round(np.cos(angle_rad)))\n",
    "        dy = int(round(np.sin(angle_rad)))\n",
    "\n",
    "        new_x = self.robot_x + dx\n",
    "        new_y = self.robot_y + dy\n",
    "\n",
    "        # Sprawdzenie granic\n",
    "        if 0 <= new_x < self.n and 0 <= new_y < self.n:\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "        else:\n",
    "            # Kara za wyjście poza obszar i zakończenie\n",
    "            self.reward -= 0.5\n",
    "            self.terminated = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseGridEnv2(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z klasycznym sterowaniem (4 akcje):\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y]\n",
    "      - Akcje: 0=góra, 1=dół, 2=lewo, 3=prawo\n",
    "\n",
    "    Dodatkowo:\n",
    "      - `grid` opisuje siatkę (n x n), w której 0 to wolne pole, 1 – pole zajęte.\n",
    "      - Wejście na pole o wartości 1 traktujemy jak kolizję.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 n=5, \n",
    "                 max_steps=100, \n",
    "                 grid=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Jeżeli przekazano siatkę, to z niej wyznacz rozmiar, w przeciwnym razie stwórz wolny obszar NxN\n",
    "        if grid is not None:\n",
    "            self.grid = np.array(grid, dtype=int)\n",
    "            self.n = self.grid.shape[0]\n",
    "        else:\n",
    "            self.n = n\n",
    "            # Domyślnie tworzymy wolną siatkę, same zera\n",
    "            self.grid = np.zeros((self.n, self.n), dtype=int)\n",
    "\n",
    "        # Przestrzeń akcji: 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Stan: (robot_x, robot_y, target_x, target_y)\n",
    "        low = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([self.n - 1, self.n - 1, self.n - 1, self.n - 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # Zmienna do śledzenia stanu (inicjalizowana w reset)\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losowanie pozycji robota (tylko na wolnym polu)\n",
    "        while True:\n",
    "            rx = np.random.randint(self.n)\n",
    "            ry = np.random.randint(self.n)\n",
    "            if self.grid[ry, rx] == 0:  # wolne pole\n",
    "                self.robot_x = rx\n",
    "                self.robot_y = ry\n",
    "                break\n",
    "\n",
    "        # Losowanie pozycji celu (tylko na wolnym polu i różne od pozycji robota)\n",
    "        while True:\n",
    "            tx = np.random.randint(self.n)\n",
    "            ty = np.random.randint(self.n)\n",
    "            if (self.grid[ty, tx] == 0 and\n",
    "                (tx != self.robot_x or ty != self.robot_y)):\n",
    "                self.target_x = tx\n",
    "                self.target_y = ty\n",
    "                break\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        # Określamy potencjalną nową pozycję\n",
    "        if action == 0:   # góra\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y - 1\n",
    "        elif action == 1: # dół\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y + 1\n",
    "        elif action == 2: # lewo\n",
    "            new_x = self.robot_x - 1\n",
    "            new_y = self.robot_y\n",
    "        elif action == 3: # prawo\n",
    "            new_x = self.robot_x + 1\n",
    "            new_y = self.robot_y\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Sprawdzenie warunków kolizji lub wyjścia poza obszar:\n",
    "        #  - wyjście poza obszar\n",
    "        #  - pole zajęte (wartość 1 w grid)\n",
    "        if not (0 <= new_x < self.n and 0 <= new_y < self.n):\n",
    "            # Kara za wyjście poza obszar\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        elif self.grid[new_y, new_x] == 1:\n",
    "            # Kara za kolizję z polem zajętym\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        else:\n",
    "            # Jeżeli pole jest wolne, aktualizujemy pozycję\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "            # Domyślna, niewielka kara za ruch\n",
    "            self.reward = -0.0001\n",
    "\n",
    "            new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                           self.robot_y - self.target_y])\n",
    "            # Nagroda/kara za przybliżenie/oddalenie się od celu\n",
    "            if new_distance < old_distance:\n",
    "                self.reward += 0.5\n",
    "            elif new_distance > old_distance:\n",
    "                self.reward -= 0.5\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Tekstowa wizualizacja środowiska:\n",
    "        'R' = robot, 'T' = cel, 'X' = przeszkoda, '.' = puste pole.\n",
    "        \"\"\"\n",
    "        grid_render = []\n",
    "        for y in range(self.n):\n",
    "            row_symbols = []\n",
    "            for x in range(self.n):\n",
    "                if self.grid[y, x] == 1:\n",
    "                    symbol = \"X\"\n",
    "                else:\n",
    "                    symbol = \".\"\n",
    "\n",
    "                if x == self.robot_x and y == self.robot_y:\n",
    "                    symbol = \"R\"\n",
    "                if x == self.target_x and y == self.target_y:\n",
    "                    symbol = \"T\"\n",
    "                row_symbols.append(symbol)\n",
    "            grid_render.append(row_symbols)\n",
    "\n",
    "        print(\"=\" * (2 * self.n))\n",
    "        for row in grid_render:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2 * self.n))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            self.robot_x, \n",
    "            self.robot_y,\n",
    "            self.target_x, \n",
    "            self.target_y\n",
    "        ], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "\n",
    "class WarehouseGridEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Magazyn NxN z klasycznym sterowaniem (4 akcje):\n",
    "      - Stan: [robot_x, robot_y, target_x, target_y]\n",
    "      - Akcje: 0=góra, 1=dół, 2=lewo, 3=prawo\n",
    "    \"\"\"\n",
    "    def __init__(self, n=5, max_steps=100):\n",
    "        super().__init__()  # wywołanie __init__ z gym.Env\n",
    "\n",
    "        self.n = n\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # Przestrzeń akcji: 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Stan: (robot_x, robot_y, target_x, target_y)\n",
    "        low = np.array([0, 0, 0, 0], dtype=np.float32)\n",
    "        high = np.array([n-1, n-1, n-1, n-1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        self.robot_x = 0\n",
    "        self.robot_y = 0\n",
    "        self.target_x = 0\n",
    "        self.target_y = 0\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = 0\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "\n",
    "        # Losowanie pozycji robota\n",
    "        self.robot_x = np.random.randint(self.n)\n",
    "        self.robot_y = np.random.randint(self.n)\n",
    "\n",
    "        # Losowanie pozycji celu\n",
    "        while True:\n",
    "            self.target_x = np.random.randint(self.n)\n",
    "            self.target_y = np.random.randint(self.n)\n",
    "            if (self.target_x != self.robot_x) or (self.target_y != self.robot_y):\n",
    "                break\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        old_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                       self.robot_y - self.target_y])\n",
    "\n",
    "        if action == 0:   # góra\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y - 1\n",
    "        elif action == 1: # dół\n",
    "            new_x = self.robot_x\n",
    "            new_y = self.robot_y + 1\n",
    "        elif action == 2: # lewo\n",
    "            new_x = self.robot_x - 1\n",
    "            new_y = self.robot_y\n",
    "        elif action == 3: # prawo\n",
    "            new_x = self.robot_x + 1\n",
    "            new_y = self.robot_y\n",
    "        else:\n",
    "            raise ValueError(f\"Nieznana akcja: {action}\")\n",
    "\n",
    "        # Sprawdzenie granic\n",
    "        if not (0 <= new_x < self.n and 0 <= new_y < self.n):\n",
    "            # Kara za wyjście poza obszar\n",
    "            self.reward = -0.5\n",
    "            self.terminated = True\n",
    "        else:\n",
    "            # Aktualizacja pozycji\n",
    "            self.robot_x = new_x\n",
    "            self.robot_y = new_y\n",
    "            # Domyślna niewielka kara za ruch\n",
    "            self.reward = -0.0001\n",
    "\n",
    "            new_distance = np.linalg.norm([self.robot_x - self.target_x,\n",
    "                                           self.robot_y - self.target_y])\n",
    "            # Nagroda/kara za przybliżenie/oddalenie\n",
    "            if new_distance < old_distance:\n",
    "                self.reward += 0.5\n",
    "            elif new_distance > old_distance:\n",
    "                self.reward -= 0.5\n",
    "\n",
    "        # Sprawdzamy, czy dotarliśmy do celu\n",
    "        if (self.robot_x == self.target_x) and (self.robot_y == self.target_y):\n",
    "            self.reward = 1.0\n",
    "            self.terminated = True\n",
    "\n",
    "        self.current_step += 1\n",
    "        truncated = (self.current_step >= self.max_steps)\n",
    "\n",
    "        return self._get_obs(), self.reward, self.terminated, truncated, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"\n",
    "        Tekstowa wizualizacja środowiska:\n",
    "        'R' = robot, 'T' = cel, '.' = puste pole.\n",
    "        \"\"\"\n",
    "        grid = [[\".\" for _ in range(self.n)] for _ in range(self.n)]\n",
    "        grid[self.robot_y][self.robot_x] = \"R\"\n",
    "        grid[self.target_y][self.target_x] = \"T\"\n",
    "\n",
    "        print(\"=\" * (2 * self.n))\n",
    "        for row in grid:\n",
    "            print(\" \".join(row))\n",
    "        print(\"=\" * (2 * self.n))\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([self.robot_x, self.robot_y,\n",
    "                         self.target_x, self.target_y], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Filip\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epizod: 1/200000, AVG ostatnich 10 ep.: 0.080, epsilon: 0.990, loss: 0.00000\n",
      "Epizod: 2/200000, AVG ostatnich 10 ep.: -0.866, epsilon: 0.980, loss: 0.00000\n",
      "Epizod: 3/200000, AVG ostatnich 10 ep.: -0.744, epsilon: 0.970, loss: 0.00000\n",
      "Epizod: 4/200000, AVG ostatnich 10 ep.: -0.788, epsilon: 0.961, loss: 0.00000\n",
      "Epizod: 5/200000, AVG ostatnich 10 ep.: -0.675, epsilon: 0.951, loss: 0.00000\n",
      "Epizod: 6/200000, AVG ostatnich 10 ep.: -0.482, epsilon: 0.941, loss: 0.00000\n",
      "Epizod: 7/200000, AVG ostatnich 10 ep.: -0.145, epsilon: 0.932, loss: 0.00000\n",
      "Epizod: 8/200000, AVG ostatnich 10 ep.: 0.071, epsilon: 0.923, loss: 0.00000\n",
      "Epizod: 9/200000, AVG ostatnich 10 ep.: -0.015, epsilon: 0.914, loss: 0.00000\n",
      "Epizod: 10/200000, AVG ostatnich 10 ep.: 0.014, epsilon: 0.904, loss: 0.00000\n",
      "Epizod: 11/200000, AVG ostatnich 10 ep.: 0.124, epsilon: 0.895, loss: 0.00000\n",
      "Epizod: 12/200000, AVG ostatnich 10 ep.: 0.284, epsilon: 0.886, loss: 0.00000\n",
      "Epizod: 13/200000, AVG ostatnich 10 ep.: 0.172, epsilon: 0.878, loss: 0.00000\n",
      "Epizod: 14/200000, AVG ostatnich 10 ep.: 0.242, epsilon: 0.869, loss: 0.00000\n",
      "Epizod: 15/200000, AVG ostatnich 10 ep.: 0.332, epsilon: 0.860, loss: 0.00000\n",
      "Epizod: 16/200000, AVG ostatnich 10 ep.: 0.213, epsilon: 0.851, loss: 0.00000\n",
      "Epizod: 17/200000, AVG ostatnich 10 ep.: -0.045, epsilon: 0.843, loss: 0.00000\n",
      "Epizod: 18/200000, AVG ostatnich 10 ep.: -0.215, epsilon: 0.835, loss: 0.00000\n",
      "Epizod: 19/200000, AVG ostatnich 10 ep.: 0.044, epsilon: 0.826, loss: 0.00000\n",
      "Epizod: 20/200000, AVG ostatnich 10 ep.: -0.066, epsilon: 0.818, loss: 0.00000\n",
      "Epizod: 21/200000, AVG ostatnich 10 ep.: -0.346, epsilon: 0.810, loss: 0.00000\n",
      "Epizod: 22/200000, AVG ostatnich 10 ep.: -0.246, epsilon: 0.802, loss: 0.00000\n",
      "Epizod: 23/200000, AVG ostatnich 10 ep.: 0.115, epsilon: 0.794, loss: 0.00000\n",
      "Epizod: 24/200000, AVG ostatnich 10 ep.: 0.046, epsilon: 0.786, loss: 0.00000\n",
      "Epizod: 25/200000, AVG ostatnich 10 ep.: 0.086, epsilon: 0.778, loss: 0.00000\n",
      "Epizod: 26/200000, AVG ostatnich 10 ep.: 0.124, epsilon: 0.770, loss: 0.00000\n",
      "Epizod: 27/200000, AVG ostatnich 10 ep.: 0.084, epsilon: 0.762, loss: 0.00000\n",
      "Epizod: 28/200000, AVG ostatnich 10 ep.: 0.154, epsilon: 0.755, loss: 0.00000\n",
      "Epizod: 29/200000, AVG ostatnich 10 ep.: 0.034, epsilon: 0.747, loss: 0.00000\n",
      "Epizod: 30/200000, AVG ostatnich 10 ep.: 0.094, epsilon: 0.740, loss: 0.00000\n",
      "Epizod: 31/200000, AVG ostatnich 10 ep.: 0.414, epsilon: 0.732, loss: 0.00000\n",
      "Epizod: 32/200000, AVG ostatnich 10 ep.: 0.274, epsilon: 0.725, loss: 0.00000\n",
      "Epizod: 33/200000, AVG ostatnich 10 ep.: -0.086, epsilon: 0.718, loss: 0.00000\n",
      "Epizod: 34/200000, AVG ostatnich 10 ep.: 0.123, epsilon: 0.711, loss: 0.00000\n",
      "Epizod: 35/200000, AVG ostatnich 10 ep.: 0.053, epsilon: 0.703, loss: 0.00000\n",
      "Epizod: 36/200000, AVG ostatnich 10 ep.: -0.006, epsilon: 0.696, loss: 0.00000\n",
      "Epizod: 37/200000, AVG ostatnich 10 ep.: 0.082, epsilon: 0.689, loss: 0.00000\n",
      "Epizod: 38/200000, AVG ostatnich 10 ep.: 0.152, epsilon: 0.683, loss: 0.00000\n",
      "Epizod: 39/200000, AVG ostatnich 10 ep.: 0.102, epsilon: 0.676, loss: 0.00000\n",
      "Epizod: 40/200000, AVG ostatnich 10 ep.: 0.063, epsilon: 0.669, loss: 0.00000\n",
      "Epizod: 41/200000, AVG ostatnich 10 ep.: 0.023, epsilon: 0.662, loss: 0.00000\n",
      "Epizod: 42/200000, AVG ostatnich 10 ep.: 0.015, epsilon: 0.656, loss: 0.00000\n",
      "Epizod: 43/200000, AVG ostatnich 10 ep.: 0.224, epsilon: 0.649, loss: 0.00000\n",
      "Epizod: 44/200000, AVG ostatnich 10 ep.: 0.015, epsilon: 0.643, loss: 0.00000\n",
      "Epizod: 45/200000, AVG ostatnich 10 ep.: -0.214, epsilon: 0.636, loss: 0.00000\n",
      "Epizod: 46/200000, AVG ostatnich 10 ep.: -0.115, epsilon: 0.630, loss: 0.00000\n",
      "Epizod: 47/200000, AVG ostatnich 10 ep.: -0.085, epsilon: 0.624, loss: 0.00000\n",
      "Epizod: 48/200000, AVG ostatnich 10 ep.: -0.205, epsilon: 0.617, loss: 0.00000\n",
      "Epizod: 49/200000, AVG ostatnich 10 ep.: -0.205, epsilon: 0.611, loss: 0.00000\n",
      "Epizod: 50/200000, AVG ostatnich 10 ep.: -0.196, epsilon: 0.605, loss: 0.00000\n",
      "Epizod: 51/200000, AVG ostatnich 10 ep.: -0.374, epsilon: 0.599, loss: 0.00000\n",
      "Epizod: 52/200000, AVG ostatnich 10 ep.: -0.396, epsilon: 0.593, loss: 0.00000\n",
      "Epizod: 53/200000, AVG ostatnich 10 ep.: -0.544, epsilon: 0.587, loss: 0.00000\n",
      "Epizod: 54/200000, AVG ostatnich 10 ep.: -0.385, epsilon: 0.581, loss: 0.00000\n",
      "Epizod: 55/200000, AVG ostatnich 10 ep.: -0.244, epsilon: 0.575, loss: 0.00000\n",
      "Epizod: 56/200000, AVG ostatnich 10 ep.: -0.303, epsilon: 0.570, loss: 0.00000\n",
      "Epizod: 57/200000, AVG ostatnich 10 ep.: -0.113, epsilon: 0.564, loss: 0.00000\n",
      "Epizod: 58/200000, AVG ostatnich 10 ep.: -0.232, epsilon: 0.558, loss: 0.00000\n",
      "Epizod: 59/200000, AVG ostatnich 10 ep.: -0.112, epsilon: 0.553, loss: 0.00000\n",
      "Epizod: 60/200000, AVG ostatnich 10 ep.: -0.110, epsilon: 0.547, loss: 0.00000\n",
      "Epizod: 61/200000, AVG ostatnich 10 ep.: -0.022, epsilon: 0.542, loss: 0.00000\n",
      "Epizod: 62/200000, AVG ostatnich 10 ep.: 0.298, epsilon: 0.536, loss: 0.00000\n",
      "Epizod: 63/200000, AVG ostatnich 10 ep.: 0.426, epsilon: 0.531, loss: 0.00000\n",
      "Epizod: 64/200000, AVG ostatnich 10 ep.: 0.366, epsilon: 0.526, loss: 0.00000\n",
      "Epizod: 65/200000, AVG ostatnich 10 ep.: 0.315, epsilon: 0.520, loss: 0.00000\n",
      "Epizod: 66/200000, AVG ostatnich 10 ep.: 0.243, epsilon: 0.515, loss: 0.00000\n",
      "Epizod: 67/200000, AVG ostatnich 10 ep.: 0.073, epsilon: 0.510, loss: 0.00000\n",
      "Epizod: 68/200000, AVG ostatnich 10 ep.: 0.133, epsilon: 0.505, loss: 0.00000\n",
      "Epizod: 69/200000, AVG ostatnich 10 ep.: 0.003, epsilon: 0.500, loss: 0.00000\n",
      "Epizod: 70/200000, AVG ostatnich 10 ep.: 0.041, epsilon: 0.495, loss: 0.00000\n",
      "Epizod: 71/200000, AVG ostatnich 10 ep.: 0.171, epsilon: 0.490, loss: 0.00000\n",
      "Epizod: 72/200000, AVG ostatnich 10 ep.: -0.129, epsilon: 0.485, loss: 0.00000\n",
      "Epizod: 73/200000, AVG ostatnich 10 ep.: -0.159, epsilon: 0.480, loss: 0.00000\n",
      "Epizod: 74/200000, AVG ostatnich 10 ep.: -0.289, epsilon: 0.475, loss: 0.00000\n",
      "Epizod: 75/200000, AVG ostatnich 10 ep.: 0.071, epsilon: 0.471, loss: 0.00000\n",
      "Epizod: 76/200000, AVG ostatnich 10 ep.: 0.431, epsilon: 0.466, loss: 0.00000\n",
      "Epizod: 77/200000, AVG ostatnich 10 ep.: 0.351, epsilon: 0.461, loss: 0.00000\n",
      "Epizod: 78/200000, AVG ostatnich 10 ep.: 0.451, epsilon: 0.457, loss: 0.00000\n",
      "Epizod: 79/200000, AVG ostatnich 10 ep.: 0.380, epsilon: 0.452, loss: 0.00000\n",
      "Epizod: 80/200000, AVG ostatnich 10 ep.: 0.272, epsilon: 0.448, loss: 0.00000\n",
      "Epizod: 81/200000, AVG ostatnich 10 ep.: 0.201, epsilon: 0.443, loss: 0.00000\n",
      "Epizod: 82/200000, AVG ostatnich 10 ep.: 0.301, epsilon: 0.439, loss: 0.00000\n",
      "Epizod: 83/200000, AVG ostatnich 10 ep.: 0.341, epsilon: 0.434, loss: 0.00000\n",
      "Epizod: 84/200000, AVG ostatnich 10 ep.: 0.362, epsilon: 0.430, loss: 0.00000\n",
      "Epizod: 85/200000, AVG ostatnich 10 ep.: -0.068, epsilon: 0.426, loss: 0.00000\n",
      "Epizod: 86/200000, AVG ostatnich 10 ep.: -0.446, epsilon: 0.421, loss: 0.00000\n",
      "Epizod: 87/200000, AVG ostatnich 10 ep.: -0.385, epsilon: 0.417, loss: 0.00000\n",
      "Epizod: 88/200000, AVG ostatnich 10 ep.: -0.385, epsilon: 0.413, loss: 0.00000\n",
      "Epizod: 89/200000, AVG ostatnich 10 ep.: -0.335, epsilon: 0.409, loss: 0.00000\n",
      "Epizod: 90/200000, AVG ostatnich 10 ep.: -0.216, epsilon: 0.405, loss: 0.00000\n",
      "Epizod: 91/200000, AVG ostatnich 10 ep.: -0.496, epsilon: 0.401, loss: 0.00000\n",
      "Epizod: 92/200000, AVG ostatnich 10 ep.: -0.626, epsilon: 0.397, loss: 0.00000\n",
      "Epizod: 93/200000, AVG ostatnich 10 ep.: -0.816, epsilon: 0.393, loss: 0.00000\n",
      "Epizod: 94/200000, AVG ostatnich 10 ep.: -0.737, epsilon: 0.389, loss: 0.00000\n",
      "Epizod: 95/200000, AVG ostatnich 10 ep.: -0.595, epsilon: 0.385, loss: 0.00000\n",
      "Epizod: 96/200000, AVG ostatnich 10 ep.: -0.525, epsilon: 0.381, loss: 0.00000\n",
      "Epizod: 97/200000, AVG ostatnich 10 ep.: -0.556, epsilon: 0.377, loss: 0.00000\n",
      "Epizod: 98/200000, AVG ostatnich 10 ep.: -0.366, epsilon: 0.373, loss: 0.00000\n",
      "Epizod: 99/200000, AVG ostatnich 10 ep.: -0.404, epsilon: 0.370, loss: 0.00000\n",
      "Epizod: 100/200000, AVG ostatnich 10 ep.: -0.614, epsilon: 0.366, loss: 0.00000\n",
      "Epizod: 101/200000, AVG ostatnich 10 ep.: -0.305, epsilon: 0.362, loss: 0.00000\n",
      "Epizod: 102/200000, AVG ostatnich 10 ep.: -0.165, epsilon: 0.359, loss: 0.00000\n",
      "Epizod: 103/200000, AVG ostatnich 10 ep.: -0.075, epsilon: 0.355, loss: 0.00000\n",
      "Epizod: 104/200000, AVG ostatnich 10 ep.: -0.164, epsilon: 0.352, loss: 0.00000\n",
      "Epizod: 105/200000, AVG ostatnich 10 ep.: 0.054, epsilon: 0.348, loss: 0.00000\n",
      "Epizod: 106/200000, AVG ostatnich 10 ep.: -0.008, epsilon: 0.345, loss: 0.00000\n",
      "Epizod: 107/200000, AVG ostatnich 10 ep.: -0.218, epsilon: 0.341, loss: 0.00000\n",
      "Epizod: 108/200000, AVG ostatnich 10 ep.: -0.508, epsilon: 0.338, loss: 0.00000\n",
      "Epizod: 109/200000, AVG ostatnich 10 ep.: -0.538, epsilon: 0.334, loss: 0.00000\n",
      "Epizod: 110/200000, AVG ostatnich 10 ep.: -0.326, epsilon: 0.331, loss: 0.00000\n",
      "Epizod: 111/200000, AVG ostatnich 10 ep.: -0.476, epsilon: 0.328, loss: 0.00000\n",
      "Epizod: 112/200000, AVG ostatnich 10 ep.: -0.396, epsilon: 0.324, loss: 0.00000\n",
      "Epizod: 113/200000, AVG ostatnich 10 ep.: -0.266, epsilon: 0.321, loss: 0.00000\n",
      "Epizod: 114/200000, AVG ostatnich 10 ep.: -0.097, epsilon: 0.318, loss: 0.00000\n",
      "Epizod: 115/200000, AVG ostatnich 10 ep.: -0.455, epsilon: 0.315, loss: 0.00000\n",
      "Epizod: 116/200000, AVG ostatnich 10 ep.: -0.295, epsilon: 0.312, loss: 0.00000\n",
      "Epizod: 117/200000, AVG ostatnich 10 ep.: -0.144, epsilon: 0.309, loss: 0.00000\n",
      "Epizod: 118/200000, AVG ostatnich 10 ep.: -0.074, epsilon: 0.305, loss: 0.00000\n",
      "Epizod: 119/200000, AVG ostatnich 10 ep.: -0.125, epsilon: 0.302, loss: 0.00000\n",
      "Epizod: 120/200000, AVG ostatnich 10 ep.: -0.067, epsilon: 0.299, loss: 0.00000\n",
      "Epizod: 121/200000, AVG ostatnich 10 ep.: 0.023, epsilon: 0.296, loss: 0.00000\n",
      "Epizod: 122/200000, AVG ostatnich 10 ep.: -0.246, epsilon: 0.293, loss: 0.00000\n",
      "Epizod: 123/200000, AVG ostatnich 10 ep.: -0.566, epsilon: 0.290, loss: 0.00000\n",
      "Epizod: 124/200000, AVG ostatnich 10 ep.: -0.566, epsilon: 0.288, loss: 0.00000\n",
      "Epizod: 125/200000, AVG ostatnich 10 ep.: -0.527, epsilon: 0.285, loss: 0.00000\n",
      "Epizod: 126/200000, AVG ostatnich 10 ep.: -0.587, epsilon: 0.282, loss: 0.00000\n",
      "Epizod: 127/200000, AVG ostatnich 10 ep.: -0.519, epsilon: 0.279, loss: 0.00000\n",
      "Epizod: 128/200000, AVG ostatnich 10 ep.: -0.559, epsilon: 0.276, loss: 0.00000\n",
      "Epizod: 129/200000, AVG ostatnich 10 ep.: -0.307, epsilon: 0.273, loss: 0.00000\n",
      "Epizod: 130/200000, AVG ostatnich 10 ep.: -0.477, epsilon: 0.271, loss: 0.00000\n",
      "Epizod: 131/200000, AVG ostatnich 10 ep.: -0.577, epsilon: 0.268, loss: 0.00000\n",
      "Epizod: 132/200000, AVG ostatnich 10 ep.: -0.348, epsilon: 0.265, loss: 0.00000\n",
      "Epizod: 133/200000, AVG ostatnich 10 ep.: -0.157, epsilon: 0.263, loss: 0.00000\n",
      "Epizod: 134/200000, AVG ostatnich 10 ep.: -0.087, epsilon: 0.260, loss: 0.00000\n",
      "Epizod: 135/200000, AVG ostatnich 10 ep.: 0.113, epsilon: 0.257, loss: 0.00000\n",
      "Epizod: 136/200000, AVG ostatnich 10 ep.: 0.053, epsilon: 0.255, loss: 0.00000\n",
      "Epizod: 137/200000, AVG ostatnich 10 ep.: 0.043, epsilon: 0.252, loss: 0.00000\n",
      "Epizod: 138/200000, AVG ostatnich 10 ep.: -0.157, epsilon: 0.250, loss: 0.00000\n",
      "Epizod: 139/200000, AVG ostatnich 10 ep.: -0.148, epsilon: 0.247, loss: 0.00000\n",
      "Epizod: 140/200000, AVG ostatnich 10 ep.: -0.138, epsilon: 0.245, loss: 0.00000\n",
      "Epizod: 141/200000, AVG ostatnich 10 ep.: -0.398, epsilon: 0.242, loss: 0.00000\n",
      "Epizod: 142/200000, AVG ostatnich 10 ep.: -0.647, epsilon: 0.240, loss: 0.00000\n",
      "Epizod: 143/200000, AVG ostatnich 10 ep.: -0.689, epsilon: 0.238, loss: 0.00000\n",
      "Epizod: 144/200000, AVG ostatnich 10 ep.: -0.779, epsilon: 0.235, loss: 0.00000\n",
      "Epizod: 145/200000, AVG ostatnich 10 ep.: -0.769, epsilon: 0.233, loss: 0.00000\n",
      "Epizod: 146/200000, AVG ostatnich 10 ep.: -0.659, epsilon: 0.231, loss: 0.00000\n",
      "Epizod: 147/200000, AVG ostatnich 10 ep.: -0.549, epsilon: 0.228, loss: 0.00000\n",
      "Epizod: 148/200000, AVG ostatnich 10 ep.: -0.358, epsilon: 0.226, loss: 0.00000\n",
      "Epizod: 149/200000, AVG ostatnich 10 ep.: -0.488, epsilon: 0.224, loss: 0.00000\n",
      "Epizod: 150/200000, AVG ostatnich 10 ep.: -0.517, epsilon: 0.221, loss: 0.00000\n",
      "Epizod: 151/200000, AVG ostatnich 10 ep.: -0.326, epsilon: 0.219, loss: 0.00000\n",
      "Epizod: 152/200000, AVG ostatnich 10 ep.: -0.028, epsilon: 0.217, loss: 0.00000\n",
      "Epizod: 153/200000, AVG ostatnich 10 ep.: -0.208, epsilon: 0.215, loss: 0.00000\n",
      "Epizod: 154/200000, AVG ostatnich 10 ep.: -0.218, epsilon: 0.213, loss: 0.00000\n",
      "Epizod: 155/200000, AVG ostatnich 10 ep.: -0.198, epsilon: 0.211, loss: 0.00000\n",
      "Epizod: 156/200000, AVG ostatnich 10 ep.: -0.247, epsilon: 0.208, loss: 0.00000\n",
      "Epizod: 157/200000, AVG ostatnich 10 ep.: -0.385, epsilon: 0.206, loss: 0.00000\n",
      "Epizod: 158/200000, AVG ostatnich 10 ep.: -0.346, epsilon: 0.204, loss: 0.00000\n",
      "Epizod: 159/200000, AVG ostatnich 10 ep.: -0.276, epsilon: 0.202, loss: 0.00000\n",
      "Epizod: 160/200000, AVG ostatnich 10 ep.: -0.027, epsilon: 0.200, loss: 0.00000\n",
      "Epizod: 161/200000, AVG ostatnich 10 ep.: -0.016, epsilon: 0.198, loss: 0.00000\n",
      "Epizod: 162/200000, AVG ostatnich 10 ep.: -0.066, epsilon: 0.196, loss: 0.00000\n",
      "Epizod: 163/200000, AVG ostatnich 10 ep.: 0.284, epsilon: 0.194, loss: 0.00000\n",
      "Epizod: 164/200000, AVG ostatnich 10 ep.: 0.094, epsilon: 0.192, loss: 0.00000\n",
      "Epizod: 165/200000, AVG ostatnich 10 ep.: 0.114, epsilon: 0.190, loss: 0.00000\n",
      "Epizod: 166/200000, AVG ostatnich 10 ep.: 0.104, epsilon: 0.189, loss: 0.00000\n",
      "Epizod: 167/200000, AVG ostatnich 10 ep.: 0.132, epsilon: 0.187, loss: 0.00000\n",
      "Epizod: 168/200000, AVG ostatnich 10 ep.: 0.013, epsilon: 0.185, loss: 0.00000\n",
      "Epizod: 169/200000, AVG ostatnich 10 ep.: -0.146, epsilon: 0.183, loss: 0.00000\n",
      "Epizod: 170/200000, AVG ostatnich 10 ep.: -0.334, epsilon: 0.181, loss: 0.00000\n",
      "Epizod: 171/200000, AVG ostatnich 10 ep.: -0.264, epsilon: 0.179, loss: 0.00000\n",
      "Epizod: 172/200000, AVG ostatnich 10 ep.: -0.442, epsilon: 0.178, loss: 0.00000\n",
      "Epizod: 173/200000, AVG ostatnich 10 ep.: -0.531, epsilon: 0.176, loss: 0.00000\n",
      "Epizod: 174/200000, AVG ostatnich 10 ep.: -0.541, epsilon: 0.174, loss: 0.00000\n",
      "Epizod: 175/200000, AVG ostatnich 10 ep.: -0.601, epsilon: 0.172, loss: 0.00000\n",
      "Epizod: 176/200000, AVG ostatnich 10 ep.: -0.771, epsilon: 0.171, loss: 0.00000\n",
      "Epizod: 177/200000, AVG ostatnich 10 ep.: -0.791, epsilon: 0.169, loss: 0.00000\n",
      "Epizod: 178/200000, AVG ostatnich 10 ep.: -0.613, epsilon: 0.167, loss: 0.00000\n",
      "Epizod: 179/200000, AVG ostatnich 10 ep.: -0.294, epsilon: 0.165, loss: 0.00000\n",
      "Epizod: 180/200000, AVG ostatnich 10 ep.: -0.166, epsilon: 0.164, loss: 0.00000\n",
      "Epizod: 181/200000, AVG ostatnich 10 ep.: 0.042, epsilon: 0.162, loss: 0.00000\n",
      "Epizod: 182/200000, AVG ostatnich 10 ep.: 0.161, epsilon: 0.161, loss: 0.00000\n",
      "Epizod: 183/200000, AVG ostatnich 10 ep.: 0.110, epsilon: 0.159, loss: 0.00000\n",
      "Epizod: 184/200000, AVG ostatnich 10 ep.: 0.250, epsilon: 0.157, loss: 0.00000\n",
      "Epizod: 185/200000, AVG ostatnich 10 ep.: 0.160, epsilon: 0.156, loss: 0.00000\n",
      "Epizod: 186/200000, AVG ostatnich 10 ep.: 0.390, epsilon: 0.154, loss: 0.00000\n",
      "Epizod: 187/200000, AVG ostatnich 10 ep.: 0.261, epsilon: 0.153, loss: 0.00000\n",
      "Epizod: 188/200000, AVG ostatnich 10 ep.: 0.171, epsilon: 0.151, loss: 0.00000\n",
      "Epizod: 189/200000, AVG ostatnich 10 ep.: -0.107, epsilon: 0.150, loss: 0.00000\n",
      "Epizod: 190/200000, AVG ostatnich 10 ep.: -0.187, epsilon: 0.148, loss: 0.00000\n",
      "Epizod: 191/200000, AVG ostatnich 10 ep.: -0.435, epsilon: 0.147, loss: 0.00000\n",
      "Epizod: 192/200000, AVG ostatnich 10 ep.: -0.574, epsilon: 0.145, loss: 0.00000\n",
      "Epizod: 193/200000, AVG ostatnich 10 ep.: -0.683, epsilon: 0.144, loss: 0.00000\n",
      "Epizod: 194/200000, AVG ostatnich 10 ep.: -0.643, epsilon: 0.142, loss: 0.00000\n",
      "Epizod: 195/200000, AVG ostatnich 10 ep.: -0.722, epsilon: 0.141, loss: 0.00000\n",
      "Epizod: 196/200000, AVG ostatnich 10 ep.: -0.810, epsilon: 0.139, loss: 0.00000\n",
      "Epizod: 197/200000, AVG ostatnich 10 ep.: -0.881, epsilon: 0.138, loss: 0.00000\n",
      "Epizod: 198/200000, AVG ostatnich 10 ep.: -0.711, epsilon: 0.137, loss: 0.00000\n",
      "Epizod: 199/200000, AVG ostatnich 10 ep.: -0.672, epsilon: 0.135, loss: 0.00000\n",
      "Epizod: 200/200000, AVG ostatnich 10 ep.: -0.582, epsilon: 0.134, loss: 0.00000\n",
      "Epizod: 201/200000, AVG ostatnich 10 ep.: -0.572, epsilon: 0.133, loss: 0.00000\n",
      "Epizod: 202/200000, AVG ostatnich 10 ep.: -0.563, epsilon: 0.131, loss: 0.00000\n",
      "Epizod: 203/200000, AVG ostatnich 10 ep.: -0.524, epsilon: 0.130, loss: 0.00000\n",
      "Epizod: 204/200000, AVG ostatnich 10 ep.: -0.484, epsilon: 0.129, loss: 0.00000\n",
      "Epizod: 205/200000, AVG ostatnich 10 ep.: -0.365, epsilon: 0.127, loss: 0.00000\n",
      "Epizod: 206/200000, AVG ostatnich 10 ep.: -0.317, epsilon: 0.126, loss: 0.00000\n",
      "Epizod: 207/200000, AVG ostatnich 10 ep.: -0.237, epsilon: 0.125, loss: 0.00000\n",
      "Epizod: 208/200000, AVG ostatnich 10 ep.: -0.527, epsilon: 0.124, loss: 0.00000\n",
      "Epizod: 209/200000, AVG ostatnich 10 ep.: -0.267, epsilon: 0.122, loss: 0.00000\n",
      "Epizod: 210/200000, AVG ostatnich 10 ep.: -0.486, epsilon: 0.121, loss: 0.00000\n",
      "Epizod: 211/200000, AVG ostatnich 10 ep.: -0.438, epsilon: 0.120, loss: 0.00000\n",
      "Epizod: 212/200000, AVG ostatnich 10 ep.: -0.338, epsilon: 0.119, loss: 0.00000\n",
      "Epizod: 213/200000, AVG ostatnich 10 ep.: -0.228, epsilon: 0.118, loss: 0.00000\n",
      "Epizod: 214/200000, AVG ostatnich 10 ep.: -0.237, epsilon: 0.116, loss: 0.00000\n",
      "Epizod: 215/200000, AVG ostatnich 10 ep.: -0.117, epsilon: 0.115, loss: 0.00000\n",
      "Epizod: 216/200000, AVG ostatnich 10 ep.: -0.047, epsilon: 0.114, loss: 0.00000\n",
      "Epizod: 217/200000, AVG ostatnich 10 ep.: 0.003, epsilon: 0.113, loss: 0.00000\n",
      "Epizod: 218/200000, AVG ostatnich 10 ep.: 0.283, epsilon: 0.112, loss: 0.00000\n",
      "Epizod: 219/200000, AVG ostatnich 10 ep.: 0.173, epsilon: 0.111, loss: 0.00000\n",
      "Epizod: 220/200000, AVG ostatnich 10 ep.: 0.281, epsilon: 0.110, loss: 0.00000\n",
      "Epizod: 221/200000, AVG ostatnich 10 ep.: 0.231, epsilon: 0.108, loss: 0.00000\n",
      "Epizod: 222/200000, AVG ostatnich 10 ep.: 0.183, epsilon: 0.107, loss: 0.00000\n",
      "Epizod: 223/200000, AVG ostatnich 10 ep.: 0.383, epsilon: 0.106, loss: 0.00000\n",
      "Epizod: 224/200000, AVG ostatnich 10 ep.: 0.153, epsilon: 0.105, loss: 0.00000\n",
      "Epizod: 225/200000, AVG ostatnich 10 ep.: -0.007, epsilon: 0.104, loss: 0.00000\n",
      "Epizod: 226/200000, AVG ostatnich 10 ep.: 0.103, epsilon: 0.103, loss: 0.00000\n",
      "Epizod: 227/200000, AVG ostatnich 10 ep.: 0.213, epsilon: 0.102, loss: 0.00000\n",
      "Epizod: 228/200000, AVG ostatnich 10 ep.: 0.123, epsilon: 0.101, loss: 0.00000\n",
      "Epizod: 229/200000, AVG ostatnich 10 ep.: -0.075, epsilon: 0.100, loss: 0.00000\n",
      "Epizod: 230/200000, AVG ostatnich 10 ep.: -0.085, epsilon: 0.099, loss: 0.00000\n",
      "Epizod: 231/200000, AVG ostatnich 10 ep.: -0.045, epsilon: 0.098, loss: 0.00000\n",
      "Epizod: 232/200000, AVG ostatnich 10 ep.: -0.116, epsilon: 0.097, loss: 0.00000\n",
      "Epizod: 233/200000, AVG ostatnich 10 ep.: -0.256, epsilon: 0.096, loss: 0.00000\n",
      "Epizod: 234/200000, AVG ostatnich 10 ep.: -0.127, epsilon: 0.095, loss: 0.00000\n",
      "Epizod: 235/200000, AVG ostatnich 10 ep.: 0.063, epsilon: 0.094, loss: 0.00000\n",
      "Epizod: 236/200000, AVG ostatnich 10 ep.: -0.167, epsilon: 0.093, loss: 0.00000\n",
      "Epizod: 237/200000, AVG ostatnich 10 ep.: -0.277, epsilon: 0.092, loss: 0.00000\n",
      "Epizod: 238/200000, AVG ostatnich 10 ep.: -0.167, epsilon: 0.091, loss: 0.00000\n",
      "Epizod: 239/200000, AVG ostatnich 10 ep.: -0.158, epsilon: 0.091, loss: 0.00000\n",
      "Epizod: 240/200000, AVG ostatnich 10 ep.: -0.148, epsilon: 0.090, loss: 0.00000\n",
      "Epizod: 241/200000, AVG ostatnich 10 ep.: -0.038, epsilon: 0.089, loss: 0.00000\n",
      "Epizod: 242/200000, AVG ostatnich 10 ep.: 0.050, epsilon: 0.088, loss: 0.00000\n",
      "Epizod: 243/200000, AVG ostatnich 10 ep.: 0.120, epsilon: 0.087, loss: 0.00000\n",
      "Epizod: 244/200000, AVG ostatnich 10 ep.: 0.300, epsilon: 0.086, loss: 0.00000\n",
      "Epizod: 245/200000, AVG ostatnich 10 ep.: 0.230, epsilon: 0.085, loss: 0.00000\n",
      "Epizod: 246/200000, AVG ostatnich 10 ep.: 0.230, epsilon: 0.084, loss: 0.00000\n",
      "Epizod: 247/200000, AVG ostatnich 10 ep.: 0.240, epsilon: 0.084, loss: 0.00000\n",
      "Epizod: 248/200000, AVG ostatnich 10 ep.: -0.060, epsilon: 0.083, loss: 0.00000\n",
      "Epizod: 249/200000, AVG ostatnich 10 ep.: -0.040, epsilon: 0.082, loss: 0.00000\n",
      "Epizod: 250/200000, AVG ostatnich 10 ep.: -0.139, epsilon: 0.081, loss: 0.00000\n",
      "Epizod: 251/200000, AVG ostatnich 10 ep.: -0.297, epsilon: 0.080, loss: 0.00000\n",
      "Epizod: 252/200000, AVG ostatnich 10 ep.: -0.325, epsilon: 0.079, loss: 0.00000\n",
      "Epizod: 253/200000, AVG ostatnich 10 ep.: -0.375, epsilon: 0.079, loss: 0.00000\n",
      "Epizod: 254/200000, AVG ostatnich 10 ep.: -0.455, epsilon: 0.078, loss: 0.00000\n",
      "Epizod: 255/200000, AVG ostatnich 10 ep.: -0.565, epsilon: 0.077, loss: 0.00000\n",
      "Epizod: 256/200000, AVG ostatnich 10 ep.: -0.495, epsilon: 0.076, loss: 0.00000\n",
      "Epizod: 257/200000, AVG ostatnich 10 ep.: -0.495, epsilon: 0.076, loss: 0.00000\n",
      "Epizod: 258/200000, AVG ostatnich 10 ep.: -0.325, epsilon: 0.075, loss: 0.00000\n",
      "Epizod: 259/200000, AVG ostatnich 10 ep.: -0.315, epsilon: 0.074, loss: 0.00000\n",
      "Epizod: 260/200000, AVG ostatnich 10 ep.: -0.246, epsilon: 0.073, loss: 0.00000\n",
      "Epizod: 261/200000, AVG ostatnich 10 ep.: -0.318, epsilon: 0.073, loss: 0.00000\n",
      "Epizod: 262/200000, AVG ostatnich 10 ep.: -0.130, epsilon: 0.072, loss: 0.00000\n",
      "Epizod: 263/200000, AVG ostatnich 10 ep.: -0.190, epsilon: 0.071, loss: 0.00000\n",
      "Epizod: 264/200000, AVG ostatnich 10 ep.: -0.150, epsilon: 0.070, loss: 0.00000\n",
      "Epizod: 265/200000, AVG ostatnich 10 ep.: -0.140, epsilon: 0.070, loss: 0.00000\n",
      "Epizod: 266/200000, AVG ostatnich 10 ep.: -0.060, epsilon: 0.069, loss: 0.00000\n",
      "Epizod: 267/200000, AVG ostatnich 10 ep.: -0.059, epsilon: 0.068, loss: 0.00000\n",
      "Epizod: 268/200000, AVG ostatnich 10 ep.: -0.209, epsilon: 0.068, loss: 0.00000\n",
      "Epizod: 269/200000, AVG ostatnich 10 ep.: -0.279, epsilon: 0.067, loss: 0.00000\n",
      "Epizod: 270/200000, AVG ostatnich 10 ep.: -0.119, epsilon: 0.066, loss: 0.00000\n",
      "Epizod: 271/200000, AVG ostatnich 10 ep.: -0.117, epsilon: 0.066, loss: 0.00000\n",
      "Epizod: 272/200000, AVG ostatnich 10 ep.: -0.177, epsilon: 0.065, loss: 0.00000\n",
      "Epizod: 273/200000, AVG ostatnich 10 ep.: -0.237, epsilon: 0.064, loss: 0.00000\n",
      "Epizod: 274/200000, AVG ostatnich 10 ep.: -0.346, epsilon: 0.064, loss: 0.00000\n",
      "Epizod: 275/200000, AVG ostatnich 10 ep.: -0.336, epsilon: 0.063, loss: 0.00000\n",
      "Epizod: 276/200000, AVG ostatnich 10 ep.: -0.196, epsilon: 0.062, loss: 0.00000\n",
      "Epizod: 277/200000, AVG ostatnich 10 ep.: -0.067, epsilon: 0.062, loss: 0.00000\n",
      "Epizod: 278/200000, AVG ostatnich 10 ep.: -0.077, epsilon: 0.061, loss: 0.00000\n",
      "Epizod: 279/200000, AVG ostatnich 10 ep.: 0.033, epsilon: 0.061, loss: 0.00000\n",
      "Epizod: 280/200000, AVG ostatnich 10 ep.: -0.327, epsilon: 0.060, loss: 0.00000\n",
      "Epizod: 281/200000, AVG ostatnich 10 ep.: -0.209, epsilon: 0.059, loss: 0.00000\n",
      "Epizod: 282/200000, AVG ostatnich 10 ep.: -0.518, epsilon: 0.059, loss: 0.00000\n",
      "Epizod: 283/200000, AVG ostatnich 10 ep.: -0.408, epsilon: 0.058, loss: 0.00000\n",
      "Epizod: 284/200000, AVG ostatnich 10 ep.: -0.428, epsilon: 0.058, loss: 0.00000\n",
      "Epizod: 285/200000, AVG ostatnich 10 ep.: -0.497, epsilon: 0.057, loss: 0.00000\n",
      "Epizod: 286/200000, AVG ostatnich 10 ep.: -0.657, epsilon: 0.056, loss: 0.00000\n",
      "Epizod: 287/200000, AVG ostatnich 10 ep.: -0.777, epsilon: 0.056, loss: 0.00000\n",
      "Epizod: 288/200000, AVG ostatnich 10 ep.: -0.687, epsilon: 0.055, loss: 0.00000\n",
      "Epizod: 289/200000, AVG ostatnich 10 ep.: -0.936, epsilon: 0.055, loss: 0.00000\n",
      "Epizod: 290/200000, AVG ostatnich 10 ep.: -0.936, epsilon: 0.054, loss: 0.00000\n",
      "Epizod: 291/200000, AVG ostatnich 10 ep.: -0.856, epsilon: 0.054, loss: 0.00000\n",
      "Epizod: 292/200000, AVG ostatnich 10 ep.: -0.566, epsilon: 0.053, loss: 0.00000\n",
      "Epizod: 293/200000, AVG ostatnich 10 ep.: -0.666, epsilon: 0.053, loss: 0.00000\n",
      "Epizod: 294/200000, AVG ostatnich 10 ep.: -0.738, epsilon: 0.052, loss: 0.00000\n",
      "Epizod: 295/200000, AVG ostatnich 10 ep.: -0.549, epsilon: 0.052, loss: 0.00000\n",
      "Epizod: 296/200000, AVG ostatnich 10 ep.: -0.599, epsilon: 0.051, loss: 0.00000\n",
      "Epizod: 297/200000, AVG ostatnich 10 ep.: -0.799, epsilon: 0.051, loss: 0.00000\n",
      "Epizod: 298/200000, AVG ostatnich 10 ep.: -0.819, epsilon: 0.050, loss: 0.00000\n",
      "Epizod: 299/200000, AVG ostatnich 10 ep.: -0.650, epsilon: 0.050, loss: 0.00000\n",
      "Epizod: 300/200000, AVG ostatnich 10 ep.: -0.478, epsilon: 0.049, loss: 0.00000\n",
      "Epizod: 301/200000, AVG ostatnich 10 ep.: -0.348, epsilon: 0.049, loss: 0.00000\n",
      "Epizod: 302/200000, AVG ostatnich 10 ep.: -0.548, epsilon: 0.048, loss: 0.00000\n",
      "Epizod: 303/200000, AVG ostatnich 10 ep.: -0.268, epsilon: 0.048, loss: 0.00000\n",
      "Epizod: 304/200000, AVG ostatnich 10 ep.: -0.058, epsilon: 0.047, loss: 0.00000\n",
      "Epizod: 305/200000, AVG ostatnich 10 ep.: -0.158, epsilon: 0.047, loss: 0.00000\n",
      "Epizod: 306/200000, AVG ostatnich 10 ep.: -0.258, epsilon: 0.046, loss: 0.00000\n",
      "Epizod: 307/200000, AVG ostatnich 10 ep.: 0.142, epsilon: 0.046, loss: 0.00000\n",
      "Epizod: 308/200000, AVG ostatnich 10 ep.: 0.272, epsilon: 0.045, loss: 0.00000\n",
      "Epizod: 309/200000, AVG ostatnich 10 ep.: 0.273, epsilon: 0.045, loss: 0.00000\n",
      "Epizod: 310/200000, AVG ostatnich 10 ep.: 0.373, epsilon: 0.044, loss: 0.00000\n",
      "Epizod: 311/200000, AVG ostatnich 10 ep.: 0.123, epsilon: 0.044, loss: 0.00000\n",
      "Epizod: 312/200000, AVG ostatnich 10 ep.: 0.413, epsilon: 0.043, loss: 0.00000\n",
      "Epizod: 313/200000, AVG ostatnich 10 ep.: 0.323, epsilon: 0.043, loss: 0.00000\n",
      "Epizod: 314/200000, AVG ostatnich 10 ep.: 0.463, epsilon: 0.043, loss: 0.00000\n",
      "Epizod: 315/200000, AVG ostatnich 10 ep.: 0.304, epsilon: 0.042, loss: 0.00000\n",
      "Epizod: 316/200000, AVG ostatnich 10 ep.: 0.474, epsilon: 0.042, loss: 0.00000\n",
      "Epizod: 317/200000, AVG ostatnich 10 ep.: 0.324, epsilon: 0.041, loss: 0.00000\n",
      "Epizod: 318/200000, AVG ostatnich 10 ep.: 0.314, epsilon: 0.041, loss: 0.00000\n",
      "Epizod: 319/200000, AVG ostatnich 10 ep.: 0.513, epsilon: 0.041, loss: 0.00000\n",
      "Epizod: 320/200000, AVG ostatnich 10 ep.: 0.662, epsilon: 0.040, loss: 0.00000\n",
      "Epizod: 321/200000, AVG ostatnich 10 ep.: 0.642, epsilon: 0.040, loss: 0.00000\n",
      "Epizod: 322/200000, AVG ostatnich 10 ep.: 0.352, epsilon: 0.039, loss: 6.58992\n",
      "Epizod: 323/200000, AVG ostatnich 10 ep.: 0.124, epsilon: 0.039, loss: 2.84502\n",
      "Epizod: 324/200000, AVG ostatnich 10 ep.: -0.204, epsilon: 0.039, loss: 1.28624\n",
      "Epizod: 325/200000, AVG ostatnich 10 ep.: -0.014, epsilon: 0.038, loss: 0.64218\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from collections import deque\n",
    "import random\n",
    "import pickle\n",
    "GAMMA = 0.9\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN = 0.0\n",
    "EPSILON_DECAY = 0.99\n",
    "BATCH_SIZE = 53164\n",
    "LEARNING_RATE = 0.0001\n",
    "MEMORY_SIZE = 20000000\n",
    "EPISODES = 200000\n",
    "def wczytaj_cyfry_z_pliku(nazwa_pliku):\n",
    "    macierz = []\n",
    "    with open(nazwa_pliku, 'r') as plik:\n",
    "        for linia in plik:\n",
    "            linia = linia.strip()\n",
    "            # Zamiast split() możemy bezpośrednio iterować po znakach w linii\n",
    "            # i konwertować je na int, o ile rzeczywiście mamy pojedyńcze cyfry\n",
    "            wiersz = [int(znak) for znak in linia]\n",
    "            macierz.append(wiersz)\n",
    "    return macierz\n",
    "my_grid=wczytaj_cyfry_z_pliku('map.txt')\n",
    "# -----------------------------------------------------------------------\n",
    "# Kluczowa rzecz: wybór środowiska, np. \"orientation\" lub \"grid\"\n",
    "# -----------------------------------------------------------------------\n",
    "ENV_TYPE = \"orientation\"  # \"orientation\" lub \"grid\"\n",
    "\n",
    "if ENV_TYPE == \"orientation\":\n",
    "    env=WarehouseOrientationEnv(n=64, alpha=90, max_steps=200)\n",
    "else:\n",
    "    my_grid =np.loadtxt('map.txt', dtype=int)\n",
    "    env =WarehouseGridEnv2(n=4, max_steps=30)\n",
    "\n",
    "# =======================================================================\n",
    "# 3. Funkcje do transformacji stanu\n",
    "# =======================================================================\n",
    "def transform_state_orientation(obs):\n",
    "    \"\"\"\n",
    "    Zakładamy obs ma postać (5,):\n",
    "      [x, y, tx, ty, orientation]   gdzie orientation ∈ {0,1,2,3}\n",
    "    Zwracamy [x, y, tx, ty, sin(θ), cos(θ)]\n",
    "    \"\"\"\n",
    "    obs = np.array(obs, dtype=float)\n",
    "    orientation = obs[4]\n",
    "    angle_deg = orientation * 90.0\n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    sin_v = np.sin(angle_rad)\n",
    "    cos_v = np.cos(angle_rad)\n",
    "    new_obs = np.concatenate([obs[:4], [sin_v, cos_v]])\n",
    "    return new_obs\n",
    "\n",
    "def transform_state_grid(obs):\n",
    "    \"\"\"\n",
    "    Zakładamy obs ma postać (4,):\n",
    "      [x, y, tx, ty]\n",
    "    Zwracamy identyczny wektor float (4,)\n",
    "    \"\"\"\n",
    "    return np.array(obs, dtype=float)\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 4. Wybieramy odpowiednią transformację + ustalamy state_size\n",
    "# =======================================================================\n",
    "if ENV_TYPE == \"orientation\":\n",
    "    transform_state_fn = transform_state_orientation\n",
    "    state_size = 6  # bo mamy sin i cos dodatkowo\n",
    "else:\n",
    "    transform_state_fn = transform_state_grid\n",
    "    state_size = 4  # (x, y, tx, ty)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 5. Definicje modelu Q-network + target network\n",
    "# =======================================================================\n",
    "def build_q_network(state_size, action_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, input_shape=(state_size,), activation='softplus'))\n",
    "    model.add(layers.Dense(64, input_shape=(state_size,), activation='softplus'))\n",
    "    model.add(layers.Dense(64, input_shape=(state_size,), activation='softplus'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "                  loss=MeanSquaredError())\n",
    "    return model\n",
    "\n",
    "def build_target_network(model):\n",
    "    target = models.clone_model(model)\n",
    "    target.set_weights(model.get_weights())\n",
    "    return target\n",
    "\n",
    "\n",
    "q_network = build_q_network(state_size, action_size)\n",
    "target_network = build_target_network(q_network)\n",
    "\n",
    "# =======================================================================\n",
    "# 6. Wczytanie pamięci z pliku (opcjonalnie) lub stworzenie pustej\n",
    "# =======================================================================\n",
    "# try:\n",
    "#     with open('memory.pkl', 'rb') as f:\n",
    "#         memory = pickle.load(f)\n",
    "#     print(\"Załadowano pamięć z pliku memory.pkl\")\n",
    "# except:\n",
    "#     print(\"Nie znaleziono pliku memory.pkl, tworzę nową pamięć.\")\n",
    "#     memory = deque(maxlen=MEMORY_SIZE)\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "epsilon = EPSILON_START\n",
    "all_rewards = []\n",
    "loss_value = 0.0\n",
    "\n",
    "# =======================================================================\n",
    "# 7. Główna pętla treningu\n",
    "# =======================================================================\n",
    "for episode in range(EPISODES):\n",
    "    obs = env.reset()                      # Oryginalny stan z env (4D lub 5D)\n",
    "    obs = transform_state_fn(obs)          # Transformacja do (4,) lub (6,)\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (terminated or truncated):\n",
    "        # Epsilon-greedy\n",
    "        if np.random.rand() < epsilon or len(memory) < BATCH_SIZE:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            q_values = q_network.predict(obs[np.newaxis, :], verbose=0)\n",
    "            action = np.argmax(q_values[0])\n",
    "\n",
    "        # Wykonanie akcji w środowisku\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_obs = transform_state_fn(next_obs)\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        # Zapis do pamięci\n",
    "        memory.append((obs, action, reward, next_obs, terminated))\n",
    "\n",
    "        # Przechodzimy do nowej obserwacji\n",
    "        obs = next_obs\n",
    "\n",
    "    # Uczenie, jeśli mamy dostatecznie dużą pamięć\n",
    "    if len(memory) >= BATCH_SIZE:\n",
    "        minibatch = random.sample(memory, BATCH_SIZE)\n",
    "        states_mb      = np.array([m[0] for m in minibatch])\n",
    "        actions_mb     = np.array([m[1] for m in minibatch])\n",
    "        rewards_mb     = np.array([m[2] for m in minibatch])\n",
    "        next_states_mb = np.array([m[3] for m in minibatch])\n",
    "        dones_mb       = np.array([m[4] for m in minibatch])\n",
    "\n",
    "        q_next = q_network.predict(next_states_mb, verbose=0)\n",
    "        q_next_target = target_network.predict(next_states_mb, verbose=0)\n",
    "        q_targets = q_network.predict(states_mb, verbose=0)\n",
    "\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones_mb[i]:\n",
    "                q_targets[i, actions_mb[i]] = rewards_mb[i]\n",
    "            else:\n",
    "                a_max = np.argmax(q_next[i])\n",
    "                q_targets[i, actions_mb[i]] = rewards_mb[i] + GAMMA * q_next_target[i, a_max]\n",
    "\n",
    "        history = q_network.fit(states_mb, q_targets, epochs=1, verbose=0)\n",
    "        loss_value = history.history['loss'][0]\n",
    "\n",
    "        # (Można wprowadzić soft-update lub twarde kopiowanie co X epizodów)\n",
    "\n",
    "    # Zmniejszamy epsilon\n",
    "    if epsilon > EPSILON_MIN:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    # Okresowe kopiowanie wag do sieci target\n",
    "    if episode % 5 == 0:\n",
    "        target_network.set_weights(q_network.get_weights())\n",
    "\n",
    "    # Monitoring\n",
    "    if (episode + 1) % 1 == 0:\n",
    "        avg_reward = np.mean(all_rewards[-10:])\n",
    "        print(f\"Epizod: {episode+1}/{EPISODES}, \"\n",
    "              f\"AVG ostatnich 10 ep.: {avg_reward:.3f}, \"\n",
    "              f\"epsilon: {epsilon:.3f}, \"\n",
    "              f\"loss: {loss_value:.5f}\")\n",
    "\n",
    "print(\"Trening zakończony!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
